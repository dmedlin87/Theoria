<!-- Generated by scripts/docs/create_agent_context.py -->

> Generated: 2025-10-25 19:16:54 UTC
> Source commit: 8f144411c9db11b77247653f87d8d2f99900cdbd
> Branch: work
> Remote: unknown
> Source files:
> - QUICK_START_FOR_AGENTS.md
> - HANDOFF_NEXT_PHASE.md
> - AGENT_HANDOFF_COMPLETE.md

---

## Primary Quick Start

## Quick Start Guide for AI Agents

This is a condensed guide for AI agents picking up development. Read this first, then refer to detailed docs as needed.

---

### What You Need to Know

#### Project Status
- **Working:** Frontend UI, RAG pipeline, pattern detection, contradiction detection, background scheduler
- **TODO:** 4 more discovery types, agent reasoning UI, dashboard, citations

#### Your Mission
Complete the discovery engine by implementing 4 remaining discovery types, then expose agent reasoning in UI.

---

### Next Task: Gap Analysis (Phase 1.2)

#### Goal
Detect missing topics in user's corpus by comparing against reference theological topics.

#### Implementation Steps

**1. Create the engine:**
```python
## File: theo/domain/discoveries/gap_engine.py

from __future__ import annotations
from dataclasses import dataclass
from typing import Sequence
from bertopic import BERTopic
from .models import DocumentEmbedding

@dataclass(frozen=True)
class GapDiscovery:
    title: str
    description: str
    missing_topic: str
    confidence: float
    relevance_score: float
    suggested_searches: list[str]
    metadata: dict[str, object]

class GapDiscoveryEngine:
    def __init__(self):
        self.topic_model = BERTopic()
        self.reference_topics = self._load_reference_topics()
    
    def _load_reference_topics(self) -> set[str]:
        # Load from data/seeds/theological_topics.yaml
        # Return set of standard topics
        pass
    
    def detect(self, documents: Sequence[DocumentEmbedding]) -> list[GapDiscovery]:
        if len(documents) < 20:  # Need minimum corpus
            return []
        
        # 1. Extract topics from user corpus
        texts = [doc.abstract or doc.title for doc in documents]
        topics, _ = self.topic_model.fit_transform(texts)
        user_topics = set(self.topic_model.get_topic_info()['Name'])
        
        # 2. Find missing topics
        missing = self.reference_topics - user_topics
        
        # 3. Create discoveries
        discoveries = []
        for topic in missing:
            discoveries.append(GapDiscovery(
                title=f"Gap detected: {topic}",
                description=f"Your corpus lacks coverage of {topic}. Consider adding resources on this topic.",
                missing_topic=topic,
                confidence=0.8,
                relevance_score=self._calculate_relevance(topic, user_topics),
                suggested_searches=[f"{topic} theology", f"biblical {topic}"],
                metadata={"user_topics": list(user_topics), "missing_topics": list(missing)}
            ))
        
        return discoveries[:20]  # Limit results
```

**2. Create reference topics:**
```yaml
## File: data/seeds/theological_topics.yaml

systematic_theology:
  - Theology Proper
  - Christology
  - Pneumatology
  - Soteriology
  - Ecclesiology
  - Eschatology
  - Anthropology
  - Hamartiology

biblical_theology:
  - Covenant Theology
  - Kingdom of God
  - Redemptive History
  - Biblical Typology

historical_theology:
  - Early Church Fathers
  - Medieval Theology
  - Reformation Theology
  - Modern Theology

practical_theology:
  - Worship
  - Preaching
  - Pastoral Care
  - Spiritual Formation
```

**3. Write tests:**
```python
## File: tests/domain/discoveries/test_gap_engine.py

import pytest
from theo.domain.discoveries.gap_engine import GapDiscoveryEngine, GapDiscovery
from theo.domain.discoveries.models import DocumentEmbedding

def test_gap_engine_requires_minimum_documents():
    engine = GapDiscoveryEngine()
    discoveries = engine.detect([])  # Too few
    assert discoveries == []

def test_gap_detection_finds_missing_topics():
    engine = GapDiscoveryEngine()
    # Create documents covering only Christology
    docs = [create_doc("Christology") for _ in range(25)]
    
    discoveries = engine.detect(docs)
    
    # Should find gaps in other topics
    assert len(discoveries) > 0
    missing_topics = [d.missing_topic for d in discoveries]
    assert "Soteriology" in missing_topics
```

**4. Integrate into service:**
```python
## File: theo/services/api/app/discoveries/service.py

from theo.domain.discoveries import (
    ContradictionDiscoveryEngine,
    GapDiscoveryEngine,  # Add this
    DiscoveryType,
    DocumentEmbedding,
    PatternDiscoveryEngine,
)

class DiscoveryService:
    def __init__(
        self,
        session: Session,
        pattern_engine: PatternDiscoveryEngine | None = None,
        contradiction_engine: ContradictionDiscoveryEngine | None = None,
        gap_engine: GapDiscoveryEngine | None = None,  # Add this
    ):
        self.session = session
        self.pattern_engine = pattern_engine or PatternDiscoveryEngine()
        self.contradiction_engine = contradiction_engine or ContradictionDiscoveryEngine()
        self.gap_engine = gap_engine or GapDiscoveryEngine()  # Add this
    
    def refresh_user_discoveries(self, user_id: str) -> list[Discovery]:
        documents = self._load_document_embeddings(user_id)
        
        # Run all engines
        pattern_candidates, snapshot = self.pattern_engine.detect(documents)
        contradiction_candidates = self.contradiction_engine.detect(documents)
        gap_candidates = self.gap_engine.detect(documents)  # Add this
        
        # Delete old discoveries
        self.session.execute(
            delete(Discovery).where(
                Discovery.user_id == user_id,
                Discovery.discovery_type.in_([
                    DiscoveryType.PATTERN.value,
                    DiscoveryType.CONTRADICTION.value,
                    DiscoveryType.GAP.value,  # Add this
                ]),
            )
        )
        
        # Persist gap discoveries
        for candidate in gap_candidates:
            record = Discovery(
                user_id=user_id,
                discovery_type=DiscoveryType.GAP.value,
                title=candidate.title,
                description=candidate.description,
                confidence=float(candidate.confidence),
                relevance_score=float(candidate.relevance_score),
                viewed=False,
                meta={
                    "missing_topic": candidate.missing_topic,
                    "suggested_searches": candidate.suggested_searches,
                    **candidate.metadata,
                },
                created_at=datetime.now(UTC),
            )
            self.session.add(record)
        
        # ... rest of method
```

**5. Add dependency:**
```txt
## File: requirements.txt
## Add this line:
bertopic>=0.15,<1
```

**6. Export from module:**
```python
## File: theo/domain/discoveries/__init__.py

from .gap_engine import GapDiscovery, GapDiscoveryEngine  # Add this

__all__ = [
    "ContradictionDiscovery",
    "ContradictionDiscoveryEngine",
    "GapDiscovery",  # Add this
    "GapDiscoveryEngine",  # Add this
    # ... rest
]
```

---

### Testing Your Work

```bash
## 1. Install dependency
pip install bertopic

## 2. Run unit tests
pytest tests/domain/discoveries/test_gap_engine.py -v

## 3. Run integration test
pytest tests/api/test_discovery_integration.py -v

## 4. Manual test
## Start services
.\start-theoria.ps1

## Upload 20+ documents
## Wait 30s for background task
## Check discoveries
curl http://localhost:8000/api/discoveries?type=gap
```

---

### After Gap Analysis

Continue with remaining discovery types in order:

1. **Connection Detection** (graph-based, shared verses)
2. **Trend Detection** (time-series, requires 3+ snapshots)
3. **Anomaly Detection** (isolation forest, outliers)

Then move to Phase 2: Expose Agent Reasoning in UI.

---

### Key Files to Reference

- `theo/domain/discoveries/contradiction_engine.py` - Just implemented, good example
- `theo/domain/discoveries/engine.py` - Pattern detection, another example
- `theo/services/api/app/discoveries/service.py` - Integration point
- `IMPLEMENTATION_CONTEXT.md` - Detailed patterns and conventions
- `HANDOFF_NEXT_PHASE.md` - Full roadmap

---

### Getting Help

All context is in the documentation:
- Architecture: `IMPLEMENTATION_CONTEXT.md`
- Roadmap: `HANDOFF_NEXT_PHASE.md`
- Session summary: `HANDOFF_SESSION_2025_10_15.md`
- Feature specs: `docs/DISCOVERY_FEATURE.md`
- Agent guide: `docs/AGENT_AND_PROMPTING_GUIDE.md`

---

**You have everything you need. Start with Gap Analysis!** 🚀

---

## Phase Roadmap

## Theoria - Next Phase Development Plan

> **Status:** Ready for implementation
> **Last Updated:** 2025-02-10
> **Estimated Timeline:** 6–8 weeks

---

### Executive Summary

The next delivery window advances the **Cognitive Scholar** initiative: durable hypothesis objects, gate-managed reasoning, and
structured debate workflows. The plan is organized around successive capability layers—gate, loop control, thought management,
debate orchestration, and prompt governance. Each phase concludes with demos and telemetry hooks to keep research UX and safety
in lockstep.

#### Current State ✅
- ✅ Frontend scaffolding for `/research/hypotheses`
- ✅ Prompt kernel prototypes for the Cognitive Gate
- ✅ Baseline Prompt-Observe-Update loop in production
- ✅ Reasoning telemetry primitives (trails, fallacy reports)
- ✅ Documentation and safety guardrails for agents

#### What's Next 🎯
1. Productionize Cognitive Gate v0 and ship hypothesis objects.
2. Swap the Prompt-Observe-Update loop to be gate-aware and log all outcomes.
3. Launch Thought Management System (TMS) v0 with persistence and admin tooling.
4. Orchestrate Debate Loop v0 and wire structured visualizations.
5. Deliver Meta-Prompt picker and finalize Beta readiness hardening.

---

### Phase 1 (Week 1–2): Hypothesis Objects & Cognitive Gate v0

#### Goals
- Persist research hypotheses with lifecycle metadata and evidence hooks.
- Stand up the Cognitive Gate service for admission control and policy enforcement.

#### Tasks
- **Hypothesis Schema & Storage**
  - Create migrations for `hypotheses`, `hypothesis_evidence`, and linking tables.
  - Implement repository/service layer APIs for CRUD and status transitions.
  - Backfill tests covering optimistic concurrency and soft-deletes.
- **Cognitive Gate Service v0**
  - Convert the prompt kernel into `theo/services/api/app/ai/gate/service.py`.
  - Implement score computation, policy thresholds, and audit logging.
  - Expose `/api/ai/gate/evaluate` endpoint with contract tests.
- **Integration & Telemetry**
  - Wire gate decisions into research/chat flows in `GuardedAnswerPipeline`.
  - Emit structured telemetry (scores, policy, overrides) to metrics stream.
  - Document gate operations, fail-open criteria, and override workflow in `docs/AGENT_AND_PROMPTING_GUIDE.md`.

### Phase 2 (Week 3): POU Loop Swap & Research UI Refresh

#### Goals
- Replace legacy evaluator with gate-managed control logic.
- Expose hypotheses and gate results in the research UI.

#### Tasks
- **POU Loop Swap**
  - Update `GuardedAnswerPipeline` to delegate to the Cognitive Gate.
  - Add fallback handling for gate errors (retry, degrade, operator alert).
  - Expand unit and integration tests to cover pass, reject, and override paths.
- **UI & API Updates**
  - Enhance `/research/hypotheses` page to display gate verdicts, scores, and timestamps.
  - Extend research API responses with hypothesis IDs and gate metadata.
  - Update docs and demos illustrating hypothesis creation and review flows.

### Phase 3 (Weeks 4–5): Thought Management System (TMS) v0

#### Goals
- Manage concurrent hypotheses, branching, and archival through a dedicated service.
- Provide operators with tooling to inspect and replay thought sequences.

#### Tasks
- **TMS Service Layer**
  - Implement `theo/services/api/app/ai/tms/service.py` with session lifecycle management.
  - Persist step history including prompt, agent, gate verdict, and outcome.
  - Create retention policy jobs for pruning inactive sessions.
- **Admin & Observability**
  - Build admin UI for TMS inspection (filter by hypothesis, gate verdict, outcome).
  - Add replay endpoint to re-run or audit sessions under new policies.
  - Instrument metrics for session duration, branch counts, and overrides.

### Phase 4 (Weeks 6): Debate Loop v0 & Visualization Layer v1

#### Goals
- Introduce orchestrated pro/con agents moderated by the Cognitive Gate.
- Visualize debates and hypothesis evolution in the research UI.

#### Tasks
- **Debate Orchestration**
  - Implement debate coordinator managing rounds, agent roles, and gate validations.
  - Persist debate transcripts linked to hypotheses and TMS sessions.
  - Establish safety guardrails, scoring rubric, and escalation paths for moderator intervention.
- **Visualization Layer v1**
  - Add timeline and graph components showing hypothesis confidence changes.
  - Surface gate/debate telemetry dashboards (admission rate, overrides, contention points).
  - Capture UX feedback checklist post-demo for iteration planning.

### Phase 5 (Week 7–8): Meta-Prompt Picker & Beta Hardening

#### Goals
- Allow researchers to select prompt presets aligned with their objectives.
- Harden the system for limited Beta release with telemetry, safety, and documentation updates.

#### Tasks
- **Meta-Prompt Picker**
  - Curate presets (Exploratory, Apologetic, Critical, Synthesis) with guardrail annotations.
  - Add UI controls to switch presets mid-session with validation warnings.
  - Track preset usage, satisfaction scores, and gate outcomes per preset.
- **Beta Hardening**
  - Run load tests for multi-hypothesis debate sessions; capture baseline metrics.
  - Expand adversarial test suite covering hallucination, citation drift, and prompt abuse.
  - Update onboarding documentation, create Beta feedback rubric, and finalize release checklist.

---

### Testing Strategy

#### Unit Tests
```bash
pytest tests/domain/hypotheses/ -v
pytest tests/api/ai/test_gate.py -v
pytest tests/api/ai/test_pou_loop.py -v
pytest tests/api/ai/test_tms.py -v
pytest tests/api/ai/test_debate.py -v
```

#### Integration & E2E
```bash
pytest tests/api/test_research_workflow.py -v
pytest tests/ui/test_hypothesis_workflow.py -v
npm run test:e2e:research --workspace theo/services/web
```

#### Telemetry & Safety Checks
```bash
python scripts/telemetry/validate_gate_events.py
pytest tests/safety/test_prompt_guardrails.py -v
```

---

### Dependencies & Risks

- **Model Variance** – Gate scoring depends on frontier models; capture calibration curves per provider.
- **Telemetry Overhead** – New metrics streams may impact ingestion; batch writes and monitor latency.
- **Operator Training** – Gate overrides and debate moderation require updated runbooks and demos.

---

### Communication Plan

- Weekly demo cadence showcasing gate decisions, TMS session replays, and debate transcripts.
- Async updates in `#theoria-research` summarizing gate metrics, hypothesis counts, and debate outcomes.
- Mid-cycle retrospective after Phase 3 to adjust scope for Beta hardening.

---

## Handoff Overview

## Agent Handoff - Complete Package

### 📦 Handoff Documents Created

Your AI agents have everything they need to continue development:

#### 1. **HANDOFF_SESSION_2025_10_15.md** - Session Summary
- What was completed this session
- Current system state (what's working, what's missing)
- Next steps in priority order
- Deployment instructions
- Testing strategy
- Configuration options

#### 2. **IMPLEMENTATION_CONTEXT.md** - Technical Guide
- Architecture patterns (hexagonal, discovery engine pattern)
- Code style & conventions (Python, TypeScript)
- Testing patterns (unit, integration)
- Database patterns (models, queries)
- API patterns (FastAPI routes, Pydantic models)
- Common pitfalls & solutions
- Development workflow
- Environment setup
- Debugging tips
- Key dependencies

#### 3. **QUICK_START_FOR_AGENTS.md** - Immediate Action Plan
- Condensed quick-start guide
- Next task: Cognitive Gate v0 + Hypothesis Objects
- Complete implementation steps with code examples
- Testing instructions
- Follow-on checklist through TMS and Debate v0

#### 4. **HANDOFF_NEXT_PHASE.md** - Long-term Roadmap
- 6-8 week Cognitive Scholar delivery plan
- Phase 1: Hypothesis Objects & Cognitive Gate v0 (Week 1-2)
- Phase 2: POU Loop Swap & Research UI Refresh (Week 3)
- Phase 3: Thought Management System (TMS) v0 (Week 4-5)
- Phase 4: Debate Loop v0 & Visualization Layer v1 (Week 6)
- Phase 5: Meta-Prompt Picker & Beta Hardening (Week 7-8)
- Success metrics for each phase

---

### ✅ What's Complete

#### This Session
1. **Background Discovery Scheduler**
   - APScheduler integration
   - Runs every 30 minutes
   - Finds active users and refreshes discoveries
   - Graceful startup/shutdown

2. **Contradiction Detection Engine**
   - NLI-based (DeBERTa-v3-base-mnli)
   - Pairwise document comparison
   - Contradiction type inference
   - Full test coverage

3. **Comprehensive Documentation**
   - Agent architecture guide
   - Scheduler documentation
   - Contradiction detection guide
   - Implementation patterns

#### Previously Complete
- Frontend UI (Next.js, complete and polished)
- RAG pipeline with guardrails
- Pattern detection (DBSCAN clustering)
- Background tasks after uploads
- Discovery API endpoints
- Database schema

---

### 🎯 What's Next (Priority Order)

#### Phase 1: Complete Discovery Engine

**1.2 Gap Analysis** ⭐ HIGH PRIORITY - START HERE
- Detect missing topics using BERTopic
- Compare against reference theological topics
- File: `theo/domain/discoveries/gap_engine.py`
- Dependency: `bertopic>=0.15,<1`

**1.3 Connection Detection** - MEDIUM PRIORITY
- Graph-based analysis of shared verses
- Find bridge documents
- File: `theo/domain/discoveries/connection_engine.py`
- Dependency: `networkx>=3.0,<4`

**1.4 Trend Detection** - MEDIUM PRIORITY
- Time-series analysis of topics
- Compare current vs historical snapshots
- File: `theo/domain/discoveries/trend_engine.py`
- Requires 3+ corpus snapshots

**1.5 Anomaly Detection** - LOW PRIORITY
- Isolation forest for outlier detection
- File: `theo/domain/discoveries/anomaly_engine.py`
- Uses sklearn (already installed)

#### Phase 2: Expose Agent Reasoning in UI

**2.1 Reasoning Mode Toggle** ⭐ HIGH PRIORITY
- Add mode selector to chat UI
- Modes: detective/critic/apologist/synthesizer
- Files: `theo/services/web/app/chat/page.tsx`, `theo/services/api/app/ai/router.py`

**2.2 Display Reasoning Trace**
- Show step-by-step reasoning
- File: `theo/services/web/components/ReasoningTrace.tsx`

**2.3 Fallacy Warnings**
- Highlight logical errors
- File: `theo/services/web/components/FallacyWarnings.tsx`

**2.4 Hypothesis Dashboard**
- Track and test hypotheses
- File: `theo/services/web/app/research/hypotheses/page.tsx`

#### Phase 3: Personalized Dashboard

- Replace landing page
- Quick stats, recent activity, discoveries
- Files: `theo/services/web/app/page.tsx`, `theo/services/api/app/routes/dashboard.py`

#### Phase 4: Citation Manager

- Export citations (APA/Chicago/SBL/BibTeX)
- Bibliography builder
- Zotero integration (optional)
- Files: `theo/services/api/app/export/citations.py`, `theo/services/web/components/CitationExport.tsx`

---

### 📚 Key Reference Files

#### Examples to Follow
- `theo/domain/discoveries/engine.py` - Pattern detection (reference implementation)
- `theo/domain/discoveries/contradiction_engine.py` - Just implemented (best example)
- `theo/services/api/app/discoveries/service.py` - Integration point

#### Documentation
- `docs/INDEX.md` - Master documentation index
- `docs/AGENT_AND_PROMPTING_GUIDE.md` - Agent architecture
- `docs/DISCOVERY_FEATURE.md` - Discovery system spec
- `docs/DISCOVERY_SCHEDULER.md` - Scheduler details
- `docs/CONTRADICTION_DETECTION.md` - Contradiction implementation

#### Existing Code
- `theo/services/api/app/main.py` - FastAPI app with scheduler
- `theo/services/api/app/routes/discoveries.py` - API endpoints
- `theo/services/web/app/discoveries/page.tsx` - Frontend UI

---

### 🚀 Quick Start for Your Agents

#### Step 1: Read Documentation
1. Start with `QUICK_START_FOR_AGENTS.md` (immediate action plan)
2. Reference `IMPLEMENTATION_CONTEXT.md` (patterns and conventions)
3. Check `HANDOFF_SESSION_2025_10_15.md` (current state)

#### Step 2: Set Up Environment
```bash
## Install dependencies
pip install -r requirements.txt

## Start services
.\start-theoria.ps1

## Verify scheduler is running
## Check logs for: "Discovery scheduler started successfully"
```

#### Step 3: Implement Gap Analysis
Follow the complete implementation in `QUICK_START_FOR_AGENTS.md`:
1. Create `theo/domain/discoveries/gap_engine.py`
2. Create `data/seeds/theological_topics.yaml`
3. Write tests in `tests/domain/discoveries/test_gap_engine.py`
4. Integrate into `DiscoveryService`
5. Add `bertopic>=0.15,<1` to `requirements.txt`
6. Export from `theo/domain/discoveries/__init__.py`

#### Step 4: Test
```bash
## Unit tests
pytest tests/domain/discoveries/test_gap_engine.py -v

## Integration tests
pytest tests/api/test_discovery_integration.py -v

## Manual test
curl http://localhost:8000/api/discoveries?type=gap
```

#### Step 5: Continue with Next Discovery Type
Repeat pattern for Connection, Trend, and Anomaly detection.

---

### 📊 Progress Tracking

#### Discovery Engine Status
- ✅ Pattern Detection (DBSCAN clustering)
- ✅ Contradiction Detection (NLI-based)
- ❌ Gap Analysis (BERTopic) ← **START HERE**
- ❌ Connection Detection (graph-based)
- ❌ Trend Detection (time-series)
- ❌ Anomaly Detection (isolation forest)

**Progress:** 2/6 complete (33%)

#### Overall Project Status
- ✅ Phase 1.1: Contradiction Detection
- ❌ Phase 1.2-1.5: Remaining discovery types
- ❌ Phase 2: Agent reasoning UI
- ❌ Phase 3: Personalized dashboard
- ❌ Phase 4: Citation manager

**Estimated Timeline:** 6-8 weeks to complete Cognitive Scholar phases

---

### 🔧 Development Workflow

#### For Each Feature
1. **Create domain logic** - Pure Python, no dependencies
2. **Write tests** - TDD approach, unit + integration
3. **Integrate into service** - Add to `DiscoveryService`
4. **Update API** - Ensure endpoints work
5. **Test manually** - Upload docs, check results
6. **Document** - Update relevant .md files

#### Code Quality
```bash
## Type checking
mypy theo/

## Linting
ruff check theo/
ruff format theo/

## Tests with coverage
pytest tests/ --cov=theo --cov-report=html
```

---

### 🎯 Success Criteria

#### Phase 1 Complete When:
- [ ] All 6 discovery types generating discoveries
- [ ] Average 10+ discoveries per user with 50+ documents
- [ ] Discovery generation < 30s for typical corpus
- [ ] All tests passing
- [ ] Documentation updated

#### Phase 2 Complete When:
- [ ] Reasoning modes selectable in chat UI
- [ ] Reasoning traces display correctly
- [ ] Fallacy warnings show up
- [ ] Hypothesis dashboard functional

#### Phase 3 Complete When:
- [ ] Dashboard replaces landing page
- [ ] Stats display correctly
- [ ] Recent activity shows
- [ ] Quick actions work

#### Phase 4 Complete When:
- [ ] Citations export in all formats
- [ ] Bibliography builder works
- [ ] Copy/download functionality works

---

### 💡 Tips for Your Agents

#### Pattern to Follow
Look at `theo/domain/discoveries/contradiction_engine.py` - it's the best example because it was just implemented. Copy its structure:

1. **Dataclass for discovery result** (frozen=True)
2. **Engine class with __init__ for config**
3. **Lazy loading for expensive resources** (_load_model pattern)
4. **detect() method that returns list of discoveries**
5. **Helper methods prefixed with _**
6. **Comprehensive docstrings**
7. **Type hints everywhere**

#### Common Patterns
- Use `from __future__ import annotations` for forward refs
- Lazy load ML models to avoid startup delays
- Batch process large datasets
- Use context managers for database sessions
- Return empty list for insufficient data (don't error)
- Sort by confidence (highest first)
- Limit results to top N (avoid overwhelming users)

#### Testing Strategy
- Test initialization
- Test with valid input
- Test with edge cases (empty, too few, invalid)
- Test with realistic data
- Mark slow tests with `@pytest.mark.slow`
- Use fixtures for test data

---

### 📞 Support

All context is in the documentation. If agents need clarification:

1. Check `IMPLEMENTATION_CONTEXT.md` for patterns
2. Look at existing engines for examples
3. Review `docs/DISCOVERY_FEATURE.md` for requirements
4. Examine tests for expected behavior

---

### ✨ Final Notes

#### What Makes This Handoff Complete

1. **Clear starting point** - Cognitive Gate v0 + Hypothesis Objects ready with full implementation guide
2. **Complete context** - Architecture, patterns, conventions all documented
3. **Working examples** - Discovery engines and reasoning flows illustrate patterns
4. **Test coverage** - Patterns and examples for testing
5. **Roadmap** - Clear path for the next 6-8 weeks of Cognitive Scholar delivery
6. **Success criteria** - Know when each Cognitive Scholar phase is done

#### Your Agents Can Now:
- ✅ Understand the architecture
- ✅ Follow established patterns
- ✅ Implement new discovery types
- ✅ Write appropriate tests
- ✅ Integrate into existing system
- ✅ Continue through all 4 phases

---

**Everything is ready. Your agents can start with Gap Analysis immediately.** 🚀

**Status:** Handoff complete  
**Next Task:** Phase 1.2 - Gap Analysis  
**Reference:** `QUICK_START_FOR_AGENTS.md`

