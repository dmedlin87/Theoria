<!-- Generated by scripts/docs/create_agent_context.py -->

> Generated: 2025-10-25 19:16:54 UTC
> Source commit: 8f144411c9db11b77247653f87d8d2f99900cdbd
> Branch: work
> Remote: unknown
> Source files:
> - QUICK_START_FOR_AGENTS.md
> - HANDOFF_NEXT_PHASE.md
> - AGENT_HANDOFF_COMPLETE.md

---

## Primary Quick Start

## Quick Start Guide for AI Agents

This is a condensed guide for AI agents picking up development. Read this first, then refer to detailed docs as needed.

---

### What You Need to Know

#### Project Status
- **Working:** Frontend UI, RAG pipeline, pattern detection, contradiction detection, background scheduler
- **TODO:** 4 more discovery types, agent reasoning UI, dashboard, citations

#### Your Mission
Complete the discovery engine by implementing 4 remaining discovery types, then expose agent reasoning in UI.

---

### Next Task: Gap Analysis (Phase 1.2)

#### Goal
Detect missing topics in user's corpus by comparing against reference theological topics.

#### Implementation Steps

**1. Create the engine:**
```python
## File: theo/domain/discoveries/gap_engine.py

from __future__ import annotations
from dataclasses import dataclass
from typing import Sequence
from bertopic import BERTopic
from .models import DocumentEmbedding

@dataclass(frozen=True)
class GapDiscovery:
    title: str
    description: str
    missing_topic: str
    confidence: float
    relevance_score: float
    suggested_searches: list[str]
    metadata: dict[str, object]

class GapDiscoveryEngine:
    def __init__(self):
        self.topic_model = BERTopic()
        self.reference_topics = self._load_reference_topics()
    
    def _load_reference_topics(self) -> set[str]:
        # Load from data/seeds/theological_topics.yaml
        # Return set of standard topics
        pass
    
    def detect(self, documents: Sequence[DocumentEmbedding]) -> list[GapDiscovery]:
        if len(documents) < 20:  # Need minimum corpus
            return []
        
        # 1. Extract topics from user corpus
        texts = [doc.abstract or doc.title for doc in documents]
        topics, _ = self.topic_model.fit_transform(texts)
        user_topics = set(self.topic_model.get_topic_info()['Name'])
        
        # 2. Find missing topics
        missing = self.reference_topics - user_topics
        
        # 3. Create discoveries
        discoveries = []
        for topic in missing:
            discoveries.append(GapDiscovery(
                title=f"Gap detected: {topic}",
                description=f"Your corpus lacks coverage of {topic}. Consider adding resources on this topic.",
                missing_topic=topic,
                confidence=0.8,
                relevance_score=self._calculate_relevance(topic, user_topics),
                suggested_searches=[f"{topic} theology", f"biblical {topic}"],
                metadata={"user_topics": list(user_topics), "missing_topics": list(missing)}
            ))
        
        return discoveries[:20]  # Limit results
```

**2. Create reference topics:**
```yaml
## File: data/seeds/theological_topics.yaml

systematic_theology:
  - Theology Proper
  - Christology
  - Pneumatology
  - Soteriology
  - Ecclesiology
  - Eschatology
  - Anthropology
  - Hamartiology

biblical_theology:
  - Covenant Theology
  - Kingdom of God
  - Redemptive History
  - Biblical Typology

historical_theology:
  - Early Church Fathers
  - Medieval Theology
  - Reformation Theology
  - Modern Theology

practical_theology:
  - Worship
  - Preaching
  - Pastoral Care
  - Spiritual Formation
```

**3. Write tests:**
```python
## File: tests/domain/discoveries/test_gap_engine.py

import pytest
from theo.domain.discoveries.gap_engine import GapDiscoveryEngine, GapDiscovery
from theo.domain.discoveries.models import DocumentEmbedding

def test_gap_engine_requires_minimum_documents():
    engine = GapDiscoveryEngine()
    discoveries = engine.detect([])  # Too few
    assert discoveries == []

def test_gap_detection_finds_missing_topics():
    engine = GapDiscoveryEngine()
    # Create documents covering only Christology
    docs = [create_doc("Christology") for _ in range(25)]
    
    discoveries = engine.detect(docs)
    
    # Should find gaps in other topics
    assert len(discoveries) > 0
    missing_topics = [d.missing_topic for d in discoveries]
    assert "Soteriology" in missing_topics
```

**4. Integrate into service:**
```python
## File: theo/services/api/app/discoveries/service.py

from theo.domain.discoveries import (
    ContradictionDiscoveryEngine,
    GapDiscoveryEngine,  # Add this
    DiscoveryType,
    DocumentEmbedding,
    PatternDiscoveryEngine,
)

class DiscoveryService:
    def __init__(
        self,
        session: Session,
        pattern_engine: PatternDiscoveryEngine | None = None,
        contradiction_engine: ContradictionDiscoveryEngine | None = None,
        gap_engine: GapDiscoveryEngine | None = None,  # Add this
    ):
        self.session = session
        self.pattern_engine = pattern_engine or PatternDiscoveryEngine()
        self.contradiction_engine = contradiction_engine or ContradictionDiscoveryEngine()
        self.gap_engine = gap_engine or GapDiscoveryEngine()  # Add this
    
    def refresh_user_discoveries(self, user_id: str) -> list[Discovery]:
        documents = self._load_document_embeddings(user_id)
        
        # Run all engines
        pattern_candidates, snapshot = self.pattern_engine.detect(documents)
        contradiction_candidates = self.contradiction_engine.detect(documents)
        gap_candidates = self.gap_engine.detect(documents)  # Add this
        
        # Delete old discoveries
        self.session.execute(
            delete(Discovery).where(
                Discovery.user_id == user_id,
                Discovery.discovery_type.in_([
                    DiscoveryType.PATTERN.value,
                    DiscoveryType.CONTRADICTION.value,
                    DiscoveryType.GAP.value,  # Add this
                ]),
            )
        )
        
        # Persist gap discoveries
        for candidate in gap_candidates:
            record = Discovery(
                user_id=user_id,
                discovery_type=DiscoveryType.GAP.value,
                title=candidate.title,
                description=candidate.description,
                confidence=float(candidate.confidence),
                relevance_score=float(candidate.relevance_score),
                viewed=False,
                meta={
                    "missing_topic": candidate.missing_topic,
                    "suggested_searches": candidate.suggested_searches,
                    **candidate.metadata,
                },
                created_at=datetime.now(UTC),
            )
            self.session.add(record)
        
        # ... rest of method
```

**5. Add dependency:**
```txt
## File: requirements.txt
## Add this line:
bertopic>=0.15,<1
```

**6. Export from module:**
```python
## File: theo/domain/discoveries/__init__.py

from .gap_engine import GapDiscovery, GapDiscoveryEngine  # Add this

__all__ = [
    "ContradictionDiscovery",
    "ContradictionDiscoveryEngine",
    "GapDiscovery",  # Add this
    "GapDiscoveryEngine",  # Add this
    # ... rest
]
```

---

### Testing Your Work

```bash
## 1. Install dependency
pip install bertopic

## 2. Run unit tests
pytest tests/domain/discoveries/test_gap_engine.py -v

## 3. Run integration test
pytest tests/api/test_discovery_integration.py -v

## 4. Manual test
## Start services
.\start-theoria.ps1

## Upload 20+ documents
## Wait 30s for background task
## Check discoveries
curl http://localhost:8000/api/discoveries?type=gap
```

---

### After Gap Analysis

Continue with remaining discovery types in order:

1. **Connection Detection** (graph-based, shared verses)
2. **Trend Detection** (time-series, requires 3+ snapshots)
3. **Anomaly Detection** (isolation forest, outliers)

Then move to Phase 2: Expose Agent Reasoning in UI.

---

### Key Files to Reference

- `theo/domain/discoveries/contradiction_engine.py` - Just implemented, good example
- `theo/domain/discoveries/engine.py` - Pattern detection, another example
- `theo/services/api/app/discoveries/service.py` - Integration point
- `IMPLEMENTATION_CONTEXT.md` - Detailed patterns and conventions
- `HANDOFF_NEXT_PHASE.md` - Full roadmap

---

### Getting Help

All context is in the documentation:
- Architecture: `IMPLEMENTATION_CONTEXT.md`
- Roadmap: `HANDOFF_NEXT_PHASE.md`
- Session summary: `HANDOFF_SESSION_2025_10_15.md`
- Feature specs: `docs/DISCOVERY_FEATURE.md`
- Agent guide: `docs/AGENT_AND_PROMPTING_GUIDE.md`

---

**You have everything you need. Start with Gap Analysis!** ðŸš€

---

## Phase Roadmap

## Theoria - Next Phase Development Plan

> **Status:** Ready for implementation
> **Last Updated:** 2025-02-10
> **Estimated Timeline:** 6â€“8 weeks

---

### Executive Summary

The next delivery window advances the **Cognitive Scholar** initiative: durable hypothesis objects, gate-managed reasoning, and
structured debate workflows. The plan is organized around successive capability layersâ€”gate, loop control, thought management,
debate orchestration, and prompt governance. Each phase concludes with demos and telemetry hooks to keep research UX and safety
in lockstep.

#### Current State âœ…
- âœ… Frontend scaffolding for `/research/hypotheses`
- âœ… Prompt kernel prototypes for the Cognitive Gate
- âœ… Baseline Prompt-Observe-Update loop in production
- âœ… Reasoning telemetry primitives (trails, fallacy reports)
- âœ… Documentation and safety guardrails for agents

#### What's Next ðŸŽ¯
1. Productionize Cognitive Gate v0 and ship hypothesis objects.
2. Swap the Prompt-Observe-Update loop to be gate-aware and log all outcomes.
3. Launch Thought Management System (TMS) v0 with persistence and admin tooling.
4. Orchestrate Debate Loop v0 and wire structured visualizations.
5. Deliver Meta-Prompt picker and finalize Beta readiness hardening.

---

### Phase 1 (Week 1â€“2): Hypothesis Objects & Cognitive Gate v0

#### Goals
- Persist research hypotheses with lifecycle metadata and evidence hooks.
- Stand up the Cognitive Gate service for admission control and policy enforcement.

#### Tasks
- **Hypothesis Schema & Storage**
  - Create migrations for `hypotheses`, `hypothesis_evidence`, and linking tables.
  - Implement repository/service layer APIs for CRUD and status transitions.
  - Backfill tests covering optimistic concurrency and soft-deletes.
- **Cognitive Gate Service v0**
  - Convert the prompt kernel into `theo/services/api/app/ai/gate/service.py`.
  - Implement score computation, policy thresholds, and audit logging.
  - Expose `/api/ai/gate/evaluate` endpoint with contract tests.
- **Integration & Telemetry**
  - Wire gate decisions into research/chat flows in `GuardedAnswerPipeline`.
  - Emit structured telemetry (scores, policy, overrides) to metrics stream.
  - Document gate operations, fail-open criteria, and override workflow in `docs/AGENT_AND_PROMPTING_GUIDE.md`.

### Phase 2 (Week 3): POU Loop Swap & Research UI Refresh

#### Goals
- Replace legacy evaluator with gate-managed control logic.
- Expose hypotheses and gate results in the research UI.

#### Tasks
- **POU Loop Swap**
  - Update `GuardedAnswerPipeline` to delegate to the Cognitive Gate.
  - Add fallback handling for gate errors (retry, degrade, operator alert).
  - Expand unit and integration tests to cover pass, reject, and override paths.
- **UI & API Updates**
  - Enhance `/research/hypotheses` page to display gate verdicts, scores, and timestamps.
  - Extend research API responses with hypothesis IDs and gate metadata.
  - Update docs and demos illustrating hypothesis creation and review flows.

### Phase 3 (Weeks 4â€“5): Thought Management System (TMS) v0

#### Goals
- Manage concurrent hypotheses, branching, and archival through a dedicated service.
- Provide operators with tooling to inspect and replay thought sequences.

#### Tasks
- **TMS Service Layer**
  - Implement `theo/services/api/app/ai/tms/service.py` with session lifecycle management.
  - Persist step history including prompt, agent, gate verdict, and outcome.
  - Create retention policy jobs for pruning inactive sessions.
- **Admin & Observability**
  - Build admin UI for TMS inspection (filter by hypothesis, gate verdict, outcome).
  - Add replay endpoint to re-run or audit sessions under new policies.
  - Instrument metrics for session duration, branch counts, and overrides.

### Phase 4 (Weeks 6): Debate Loop v0 & Visualization Layer v1

#### Goals
- Introduce orchestrated pro/con agents moderated by the Cognitive Gate.
- Visualize debates and hypothesis evolution in the research UI.

#### Tasks
- **Debate Orchestration**
  - Implement debate coordinator managing rounds, agent roles, and gate validations.
  - Persist debate transcripts linked to hypotheses and TMS sessions.
  - Establish safety guardrails, scoring rubric, and escalation paths for moderator intervention.
- **Visualization Layer v1**
  - Add timeline and graph components showing hypothesis confidence changes.
  - Surface gate/debate telemetry dashboards (admission rate, overrides, contention points).
  - Capture UX feedback checklist post-demo for iteration planning.

### Phase 5 (Week 7â€“8): Meta-Prompt Picker & Beta Hardening

#### Goals
- Allow researchers to select prompt presets aligned with their objectives.
- Harden the system for limited Beta release with telemetry, safety, and documentation updates.

#### Tasks
- **Meta-Prompt Picker**
  - Curate presets (Exploratory, Apologetic, Critical, Synthesis) with guardrail annotations.
  - Add UI controls to switch presets mid-session with validation warnings.
  - Track preset usage, satisfaction scores, and gate outcomes per preset.
- **Beta Hardening**
  - Run load tests for multi-hypothesis debate sessions; capture baseline metrics.
  - Expand adversarial test suite covering hallucination, citation drift, and prompt abuse.
  - Update onboarding documentation, create Beta feedback rubric, and finalize release checklist.

---

### Testing Strategy

#### Unit Tests
```bash
pytest tests/domain/hypotheses/ -v
pytest tests/api/ai/test_gate.py -v
pytest tests/api/ai/test_pou_loop.py -v
pytest tests/api/ai/test_tms.py -v
pytest tests/api/ai/test_debate.py -v
```

#### Integration & E2E
```bash
pytest tests/api/test_research_workflow.py -v
pytest tests/ui/test_hypothesis_workflow.py -v
npm run test:e2e:research --workspace theo/services/web
```

#### Telemetry & Safety Checks
```bash
python scripts/telemetry/validate_gate_events.py
pytest tests/safety/test_prompt_guardrails.py -v
```

---

### Dependencies & Risks

- **Model Variance** â€“ Gate scoring depends on frontier models; capture calibration curves per provider.
- **Telemetry Overhead** â€“ New metrics streams may impact ingestion; batch writes and monitor latency.
- **Operator Training** â€“ Gate overrides and debate moderation require updated runbooks and demos.

---

### Communication Plan

- Weekly demo cadence showcasing gate decisions, TMS session replays, and debate transcripts.
- Async updates in `#theoria-research` summarizing gate metrics, hypothesis counts, and debate outcomes.
- Mid-cycle retrospective after Phase 3 to adjust scope for Beta hardening.

---

## Handoff Overview

## Agent Handoff - Complete Package

### ðŸ“¦ Handoff Documents Created

Your AI agents have everything they need to continue development:

#### 1. **HANDOFF_SESSION_2025_10_15.md** - Session Summary
- What was completed this session
- Current system state (what's working, what's missing)
- Next steps in priority order
- Deployment instructions
- Testing strategy
- Configuration options

#### 2. **IMPLEMENTATION_CONTEXT.md** - Technical Guide
- Architecture patterns (hexagonal, discovery engine pattern)
- Code style & conventions (Python, TypeScript)
- Testing patterns (unit, integration)
- Database patterns (models, queries)
- API patterns (FastAPI routes, Pydantic models)
- Common pitfalls & solutions
- Development workflow
- Environment setup
- Debugging tips
- Key dependencies

#### 3. **QUICK_START_FOR_AGENTS.md** - Immediate Action Plan
- Condensed quick-start guide
- Next task: Cognitive Gate v0 + Hypothesis Objects
- Complete implementation steps with code examples
- Testing instructions
- Follow-on checklist through TMS and Debate v0

#### 4. **HANDOFF_NEXT_PHASE.md** - Long-term Roadmap
- 6-8 week Cognitive Scholar delivery plan
- Phase 1: Hypothesis Objects & Cognitive Gate v0 (Week 1-2)
- Phase 2: POU Loop Swap & Research UI Refresh (Week 3)
- Phase 3: Thought Management System (TMS) v0 (Week 4-5)
- Phase 4: Debate Loop v0 & Visualization Layer v1 (Week 6)
- Phase 5: Meta-Prompt Picker & Beta Hardening (Week 7-8)
- Success metrics for each phase

---

### âœ… What's Complete

#### This Session
1. **Background Discovery Scheduler**
   - APScheduler integration
   - Runs every 30 minutes
   - Finds active users and refreshes discoveries
   - Graceful startup/shutdown

2. **Contradiction Detection Engine**
   - NLI-based (DeBERTa-v3-base-mnli)
   - Pairwise document comparison
   - Contradiction type inference
   - Full test coverage

3. **Comprehensive Documentation**
   - Agent architecture guide
   - Scheduler documentation
   - Contradiction detection guide
   - Implementation patterns

#### Previously Complete
- Frontend UI (Next.js, complete and polished)
- RAG pipeline with guardrails
- Pattern detection (DBSCAN clustering)
- Background tasks after uploads
- Discovery API endpoints
- Database schema

---

### ðŸŽ¯ What's Next (Priority Order)

#### Phase 1: Complete Discovery Engine

**1.2 Gap Analysis** â­ HIGH PRIORITY - START HERE
- Detect missing topics using BERTopic
- Compare against reference theological topics
- File: `theo/domain/discoveries/gap_engine.py`
- Dependency: `bertopic>=0.15,<1`

**1.3 Connection Detection** - MEDIUM PRIORITY
- Graph-based analysis of shared verses
- Find bridge documents
- File: `theo/domain/discoveries/connection_engine.py`
- Dependency: `networkx>=3.0,<4`

**1.4 Trend Detection** - MEDIUM PRIORITY
- Time-series analysis of topics
- Compare current vs historical snapshots
- File: `theo/domain/discoveries/trend_engine.py`
- Requires 3+ corpus snapshots

**1.5 Anomaly Detection** - LOW PRIORITY
- Isolation forest for outlier detection
- File: `theo/domain/discoveries/anomaly_engine.py`
- Uses sklearn (already installed)

#### Phase 2: Expose Agent Reasoning in UI

**2.1 Reasoning Mode Toggle** â­ HIGH PRIORITY
- Add mode selector to chat UI
- Modes: detective/critic/apologist/synthesizer
- Files: `theo/services/web/app/chat/page.tsx`, `theo/services/api/app/ai/router.py`

**2.2 Display Reasoning Trace**
- Show step-by-step reasoning
- File: `theo/services/web/components/ReasoningTrace.tsx`

**2.3 Fallacy Warnings**
- Highlight logical errors
- File: `theo/services/web/components/FallacyWarnings.tsx`

**2.4 Hypothesis Dashboard**
- Track and test hypotheses
- File: `theo/services/web/app/research/hypotheses/page.tsx`

#### Phase 3: Personalized Dashboard

- Replace landing page
- Quick stats, recent activity, discoveries
- Files: `theo/services/web/app/page.tsx`, `theo/services/api/app/routes/dashboard.py`

#### Phase 4: Citation Manager

- Export citations (APA/Chicago/SBL/BibTeX)
- Bibliography builder
- Zotero integration (optional)
- Files: `theo/services/api/app/export/citations.py`, `theo/services/web/components/CitationExport.tsx`

---

### ðŸ“š Key Reference Files

#### Examples to Follow
- `theo/domain/discoveries/engine.py` - Pattern detection (reference implementation)
- `theo/domain/discoveries/contradiction_engine.py` - Just implemented (best example)
- `theo/services/api/app/discoveries/service.py` - Integration point

#### Documentation
- `docs/INDEX.md` - Master documentation index
- `docs/AGENT_AND_PROMPTING_GUIDE.md` - Agent architecture
- `docs/DISCOVERY_FEATURE.md` - Discovery system spec
- `docs/DISCOVERY_SCHEDULER.md` - Scheduler details
- `docs/CONTRADICTION_DETECTION.md` - Contradiction implementation

#### Existing Code
- `theo/services/api/app/main.py` - FastAPI app with scheduler
- `theo/services/api/app/routes/discoveries.py` - API endpoints
- `theo/services/web/app/discoveries/page.tsx` - Frontend UI

---

### ðŸš€ Quick Start for Your Agents

#### Step 1: Read Documentation
1. Start with `QUICK_START_FOR_AGENTS.md` (immediate action plan)
2. Reference `IMPLEMENTATION_CONTEXT.md` (patterns and conventions)
3. Check `HANDOFF_SESSION_2025_10_15.md` (current state)

#### Step 2: Set Up Environment
```bash
## Install dependencies
pip install -r requirements.txt

## Start services
.\start-theoria.ps1

## Verify scheduler is running
## Check logs for: "Discovery scheduler started successfully"
```

#### Step 3: Implement Gap Analysis
Follow the complete implementation in `QUICK_START_FOR_AGENTS.md`:
1. Create `theo/domain/discoveries/gap_engine.py`
2. Create `data/seeds/theological_topics.yaml`
3. Write tests in `tests/domain/discoveries/test_gap_engine.py`
4. Integrate into `DiscoveryService`
5. Add `bertopic>=0.15,<1` to `requirements.txt`
6. Export from `theo/domain/discoveries/__init__.py`

#### Step 4: Test
```bash
## Unit tests
pytest tests/domain/discoveries/test_gap_engine.py -v

## Integration tests
pytest tests/api/test_discovery_integration.py -v

## Manual test
curl http://localhost:8000/api/discoveries?type=gap
```

#### Step 5: Continue with Next Discovery Type
Repeat pattern for Connection, Trend, and Anomaly detection.

---

### ðŸ“Š Progress Tracking

#### Discovery Engine Status
- âœ… Pattern Detection (DBSCAN clustering)
- âœ… Contradiction Detection (NLI-based)
- âŒ Gap Analysis (BERTopic) â† **START HERE**
- âŒ Connection Detection (graph-based)
- âŒ Trend Detection (time-series)
- âŒ Anomaly Detection (isolation forest)

**Progress:** 2/6 complete (33%)

#### Overall Project Status
- âœ… Phase 1.1: Contradiction Detection
- âŒ Phase 1.2-1.5: Remaining discovery types
- âŒ Phase 2: Agent reasoning UI
- âŒ Phase 3: Personalized dashboard
- âŒ Phase 4: Citation manager

**Estimated Timeline:** 6-8 weeks to complete Cognitive Scholar phases

---

### ðŸ”§ Development Workflow

#### For Each Feature
1. **Create domain logic** - Pure Python, no dependencies
2. **Write tests** - TDD approach, unit + integration
3. **Integrate into service** - Add to `DiscoveryService`
4. **Update API** - Ensure endpoints work
5. **Test manually** - Upload docs, check results
6. **Document** - Update relevant .md files

#### Code Quality
```bash
## Type checking
mypy theo/

## Linting
ruff check theo/
ruff format theo/

## Tests with coverage
pytest tests/ --cov=theo --cov-report=html
```

---

### ðŸŽ¯ Success Criteria

#### Phase 1 Complete When:
- [ ] All 6 discovery types generating discoveries
- [ ] Average 10+ discoveries per user with 50+ documents
- [ ] Discovery generation < 30s for typical corpus
- [ ] All tests passing
- [ ] Documentation updated

#### Phase 2 Complete When:
- [ ] Reasoning modes selectable in chat UI
- [ ] Reasoning traces display correctly
- [ ] Fallacy warnings show up
- [ ] Hypothesis dashboard functional

#### Phase 3 Complete When:
- [ ] Dashboard replaces landing page
- [ ] Stats display correctly
- [ ] Recent activity shows
- [ ] Quick actions work

#### Phase 4 Complete When:
- [ ] Citations export in all formats
- [ ] Bibliography builder works
- [ ] Copy/download functionality works

---

### ðŸ’¡ Tips for Your Agents

#### Pattern to Follow
Look at `theo/domain/discoveries/contradiction_engine.py` - it's the best example because it was just implemented. Copy its structure:

1. **Dataclass for discovery result** (frozen=True)
2. **Engine class with __init__ for config**
3. **Lazy loading for expensive resources** (_load_model pattern)
4. **detect() method that returns list of discoveries**
5. **Helper methods prefixed with _**
6. **Comprehensive docstrings**
7. **Type hints everywhere**

#### Common Patterns
- Use `from __future__ import annotations` for forward refs
- Lazy load ML models to avoid startup delays
- Batch process large datasets
- Use context managers for database sessions
- Return empty list for insufficient data (don't error)
- Sort by confidence (highest first)
- Limit results to top N (avoid overwhelming users)

#### Testing Strategy
- Test initialization
- Test with valid input
- Test with edge cases (empty, too few, invalid)
- Test with realistic data
- Mark slow tests with `@pytest.mark.slow`
- Use fixtures for test data

---

### ðŸ“ž Support

All context is in the documentation. If agents need clarification:

1. Check `IMPLEMENTATION_CONTEXT.md` for patterns
2. Look at existing engines for examples
3. Review `docs/DISCOVERY_FEATURE.md` for requirements
4. Examine tests for expected behavior

---

### âœ¨ Final Notes

#### What Makes This Handoff Complete

1. **Clear starting point** - Cognitive Gate v0 + Hypothesis Objects ready with full implementation guide
2. **Complete context** - Architecture, patterns, conventions all documented
3. **Working examples** - Discovery engines and reasoning flows illustrate patterns
4. **Test coverage** - Patterns and examples for testing
5. **Roadmap** - Clear path for the next 6-8 weeks of Cognitive Scholar delivery
6. **Success criteria** - Know when each Cognitive Scholar phase is done

#### Your Agents Can Now:
- âœ… Understand the architecture
- âœ… Follow established patterns
- âœ… Implement new discovery types
- âœ… Write appropriate tests
- âœ… Integrate into existing system
- âœ… Continue through all 4 phases

---

**Everything is ready. Your agents can start with Gap Analysis immediately.** ðŸš€

**Status:** Handoff complete  
**Next Task:** Phase 1.2 - Gap Analysis  
**Reference:** `QUICK_START_FOR_AGENTS.md`

