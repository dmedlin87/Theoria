[
  {
    "branch": "origin/work",
    "commit": "feat: add Model Context Protocol (MCP) server with Docker and dev script support\n",
    "commitHash": "5d5f3ba8208ab1305dd1dddc654564895f440422",
    "date": "2025-10-09 23:04:29",
    "diff": "@@ -47,28 +47,6 @@ services:\n     volumes:\n       - storage:/data/storage\n \n-  mcp:\n-    build:\n-      context: ..\n-      dockerfile: mcp_server/Dockerfile\n-    env_file: ../.env\n-    environment:\n-      storage_root: /data/storage\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\n-      redis_url: redis://redis:6379/0\n-      MCP_TOOLS_ENABLED: \"1\"\n-      MCP_HOST: 0.0.0.0\n-      MCP_PORT: 8050\n-    depends_on:\n-      db:\n-        condition: service_healthy\n-      redis:\n-        condition: service_healthy\n-    ports:\n-      - \"8050:8050\"\n-    volumes:\n-      - storage:/data/storage\n-\n   web:\n     build:\n       context: ..\n@@ -85,4 +63,4 @@ services:\n \n volumes:\n   db: {}\n-  storage: {}\n+  storage: {}\n\\ No newline at end of file\n",
    "path": "infra/docker-compose.yml",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "feat: enhance Docker setup, improve API routes, and update environment configurations\n",
    "commitHash": "d547bd31ca798ef6f1637c5527a7b79343d77543",
    "date": "2025-09-28 15:47:26",
    "diff": "@@ -1,11 +1,13 @@\n # TheoEngine environment variables (example)\n-# Copy to .env and adjust for your environment.\n+# Copy to .env and adjust.\n # pwsh: Copy-Item .env.example .env -Force\n \n # --- API (FastAPI) ---\n-# Local development defaults (SQLite + local storage)\n+# Local dev: SQLite + local storage directory\n database_url=sqlite:///./theo.db\n storage_root=./storage\n+\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\n redis_url=redis://localhost:6379/0\n \n # Ingestion/embedding defaults\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\n # Optional fixtures path override (auto-detected if ./fixtures exists)\n # fixtures_root=./fixtures\n \n-# --- API authentication toggles (optional) ---\n-# THEO_API_KEYS=alpha,beta\n-# THEO_AUTH_JWT_SECRET=change-me\n-# THEO_AUTH_JWT_AUDIENCE=theo\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\n-\n # --- Web (Next.js) ---\n # Point the UI to the API in local dev\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\n API_BASE_URL=http://127.0.0.1:8000\n \n-# --- Docker Compose ---\n-# docker compose (in ./infra) reads this same file and overrides the core\n-# connection settings internally. Leave these defaults in place unless you\n-# are running the database or broker elsewhere.\n-# During compose runs the api service sets:\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\n-#   redis_url=redis://redis:6379/0\n-#   storage_root=/data/storage\n-# and the web service sets API urls to http://api:8000.\n+# --- Docker Compose variants (uncomment if using compose) ---\n+# Inside the compose network, reference services by name\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\n+# API_BASE_URL=http://api:8000\n+\n+# For API using Postgres and Redis services in compose\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\n+# redis_url=redis://redis:6379/0\n+\n+# If running API in a container and mounting a data volume\n+# storage_root=/data/storage\n",
    "path": ".env.example",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n",
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "feat: enhance Docker setup, improve API routes, and update environment configurations\n",
    "commitHash": "d547bd31ca798ef6f1637c5527a7b79343d77543",
    "date": "2025-09-28 15:47:26",
    "diff": "@@ -1,66 +1,32 @@\n version: \"3.9\"\n-\n services:\n   db:\n     image: postgres:15\n     environment:\n       POSTGRES_DB: theo\n-      POSTGRES_USER: postgres\n       POSTGRES_PASSWORD: postgres\n     ports:\n       - \"5432:5432\"\n     volumes:\n       - db:/var/lib/postgresql/data\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\n-    healthcheck:\n-      test: [\"CMD-SHELL\", \"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\"]\n-      interval: 10s\n-      timeout: 5s\n-      retries: 5\n-\n   redis:\n     image: redis:7\n     ports:\n       - \"6379:6379\"\n-    healthcheck:\n-      test: [\"CMD\", \"redis-cli\", \"PING\"]\n-      interval: 10s\n-      timeout: 5s\n-      retries: 5\n-\n   api:\n-    build:\n-      context: ..\n-      dockerfile: theo/services/api/Dockerfile\n+    build: ../services/api\n     env_file: ../.env\n-    environment:\n-      storage_root: /data/storage\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\n-      redis_url: redis://redis:6379/0\n     depends_on:\n-      db:\n-        condition: service_healthy\n-      redis:\n-        condition: service_healthy\n+      - db\n+      - redis\n     ports:\n       - \"8000:8000\"\n-    volumes:\n-      - storage:/data/storage\n-\n   web:\n-    build:\n-      context: ..\n-      dockerfile: theo/services/web/Dockerfile\n+    build: ../services/web\n     env_file: ../.env\n-    environment:\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\n-      API_BASE_URL: http://api:8000\n     depends_on:\n-      api:\n-        condition: service_started\n+      - api\n     ports:\n       - \"3000:3000\"\n-\n volumes:\n   db: {}\n-  storage: {}\n\\ No newline at end of file\n",
    "path": "infra/docker-compose.yml",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "Refactor example environment variables for clarity and local development setup\n",
    "commitHash": "93cc7b51d2e05d903f2d4f28544224a25ee7fe79",
    "date": "2025-09-24 13:17:03",
    "diff": "@@ -1,39 +1,3 @@\n-# TheoEngine environment variables (example)\n-# Copy to .env and adjust.\n-# pwsh: Copy-Item .env.example .env -Force\n-\n-# --- API (FastAPI) ---\n-# Local dev: SQLite + local storage directory\n-database_url=sqlite:///./theo.db\n-storage_root=./storage\n-\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\n-redis_url=redis://localhost:6379/0\n-\n-# Ingestion/embedding defaults\n-embedding_model=BAAI/bge-m3\n-embedding_dim=1024\n-max_chunk_tokens=900\n-doc_max_pages=5000\n-transcript_max_window=40.0\n-user_agent=TheoEngine/1.0\n-\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\n-# fixtures_root=./fixtures\n-\n-# --- Web (Next.js) ---\n-# Point the UI to the API in local dev\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\n-API_BASE_URL=http://127.0.0.1:8000\n-\n-# --- Docker Compose variants (uncomment if using compose) ---\n-# Inside the compose network, reference services by name\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\n-# API_BASE_URL=http://api:8000\n-\n-# For API using Postgres and Redis services in compose\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\n-# redis_url=redis://redis:6379/0\n-\n-# If running API in a container and mounting a data volume\n-# storage_root=/data/storage\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\n+REDIS_URL=redis://redis:6379/0\n+STORAGE_ROOT=/data/storage\n",
    "path": ".env.example",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n",
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "implement ingestion pipeline and hybrid search",
    "commitHash": "8d1ab58da8e55381b94425a7dffc86d6a9c75437",
    "date": "2025-09-23 19:43:10",
    "diff": "@@ -1,35 +1,12 @@\n-\"\"\"Application configuration for the Theo Engine API.\"\"\"\n+\"\"\"Application configuration.\"\"\"\n \n-from functools import lru_cache\n-from pathlib import Path\n-\n-from pydantic import Field\n-from pydantic_settings import BaseSettings, SettingsConfigDict\n+from pydantic import BaseSettings, Field\n \n \n class Settings(BaseSettings):\n-    \"\"\"Runtime configuration loaded from environment variables.\"\"\"\n-\n-    model_config = SettingsConfigDict(env_prefix=\"\", env_file=\".env\", env_file_encoding=\"utf-8\")\n-\n-    database_url: str = Field(\n-        default=\"sqlite:///./theo.db\", description=\"SQLAlchemy database URL\"\n-    )\n-    redis_url: str = Field(default=\"redis://redis:6379/0\", description=\"Celery broker URL\")\n-    storage_root: Path = Field(default=Path(\"./storage\"), description=\"Location for persisted artifacts\")\n-    embedding_model: str = Field(default=\"BAAI/bge-m3\")\n-    embedding_dim: int = Field(default=1024)\n-    max_chunk_tokens: int = Field(default=900)\n-    doc_max_pages: int = Field(default=5000)\n-    user_agent: str = Field(default=\"TheoEngine/1.0\")\n-\n-@lru_cache\n-def get_settings() -> Settings:\n-    \"\"\"Return a cached Settings instance.\"\"\"\n-\n-    settings = Settings()\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\n-    return settings\n+    database_url: str = Field(env=\"DATABASE_URL\", default=\"postgresql+psycopg://postgres:postgres@db:5432/theo\")\n+    redis_url: str = Field(env=\"REDIS_URL\", default=\"redis://redis:6379/0\")\n+    storage_root: str = Field(env=\"STORAGE_ROOT\", default=\"/data/storage\")\n \n \n-settings = get_settings()\n+settings = Settings()\n",
    "path": "theo/infrastructure/api/app/core/settings.py",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\")\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\")\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "scaffold theo engine mvp structure",
    "commitHash": "6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910",
    "date": "2025-09-23 19:17:01",
    "diff": "@@ -1,3 +0,0 @@\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\n-REDIS_URL=redis://redis:6379/0\n-STORAGE_ROOT=/data/storage\n",
    "path": ".env.example",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "scaffold theo engine mvp structure",
    "commitHash": "6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910",
    "date": "2025-09-23 19:17:01",
    "diff": "@@ -1,12 +0,0 @@\n-\"\"\"Application configuration.\"\"\"\n-\n-from pydantic import BaseSettings, Field\n-\n-\n-class Settings(BaseSettings):\n-    database_url: str = Field(env=\"DATABASE_URL\", default=\"postgresql+psycopg://postgres:postgres@db:5432/theo\")\n-    redis_url: str = Field(env=\"REDIS_URL\", default=\"redis://redis:6379/0\")\n-    storage_root: str = Field(env=\"STORAGE_ROOT\", default=\"/data/storage\")\n-\n-\n-settings = Settings()\n",
    "path": "theo/infrastructure/api/app/core/settings.py",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\")\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\")\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\n",
    "commitHash": "e78da1f51a29708b06c70056b0485588372e368f",
    "date": "2025-09-23 19:04:10",
    "diff": "@@ -0,0 +1,458 @@\n+# Theo Engine \u2014 Final Build Spec (Standalone)\n+\n+## 0) Mission & MVP\n+\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\n+\n+MVP outcomes (no LLM required):\n+\n+Ingest local files and URLs (including YouTube).\n+\n+Parse to chunked, citation-preserving passages with page/time anchors.\n+\n+Detect and normalize Bible references \u2192 OSIS.\n+\n+Hybrid search (pgvector embeddings + lexical).\n+\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \u2192 see every snippet across the corpus, with jump links (page/time).\n+\n+Minimal web UI: Upload, Search, Verse, Document.\n+\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\n+\n+## 1) Repo Layout (monorepo)\n+\n+theo/\n+\u251c\u2500 services/\n+\u2502  \u251c\u2500 api/                 # FastAPI service\n+\u2502  \u2502  \u251c\u2500 app/\n+\u2502  \u2502  \u2502  \u251c\u2500 main.py\n+\u2502  \u2502  \u2502  \u251c\u2500 routes/        # FastAPI routers\n+\u2502  \u2502  \u2502  \u2502  \u251c\u2500 ingest.py\n+\u2502  \u2502  \u2502  \u2502  \u251c\u2500 search.py\n+\u2502  \u2502  \u2502  \u2502  \u251c\u2500 verses.py\n+\u2502  \u2502  \u2502  \u2502  \u2514\u2500 documents.py\n+\u2502  \u2502  \u2502  \u251c\u2500 core/          # db, settings, logging\n+\u2502  \u2502  \u2502  \u251c\u2500 ingest/        # parsers, chunkers, osis detection\n+\u2502  \u2502  \u2502  \u251c\u2500 retriever/     # hybrid search/rerank\n+\u2502  \u2502  \u2502  \u251c\u2500 models/        # pydantic schemas\n+\u2502  \u2502  \u2502  \u2514\u2500 workers/       # Celery tasks\n+\u2502  \u2502  \u2514\u2500 requirements.txt\n+\u2502  \u251c\u2500 web/                 # Next.js 14 (App Router)\n+\u2502  \u2502  \u251c\u2500 app/\n+\u2502  \u2502  \u2502  \u251c\u2500 upload/page.tsx\n+\u2502  \u2502  \u2502  \u251c\u2500 search/page.tsx\n+\u2502  \u2502  \u2502  \u251c\u2500 verse/[osis]/page.tsx\n+\u2502  \u2502  \u2502  \u2514\u2500 doc/[id]/page.tsx\n+\u2502  \u2502  \u2514\u2500 package.json\n+\u2502  \u2514\u2500 cli/                 # optional: bulk ingest CLI\n+\u2502     \u2514\u2500 ingest_folder.py\n+\u251c\u2500 infra/\n+\u2502  \u251c\u2500 docker-compose.yml\n+\u2502  \u251c\u2500 db-init/pgvector.sql\n+\u2502  \u2514\u2500 Makefile\n+\u251c\u2500 docs/\n+\u2502  \u251c\u2500 API.md\n+\u2502  \u251c\u2500 Chunking.md\n+\u2502  \u251c\u2500 OSIS.md\n+\u2502  \u2514\u2500 Frontmatter.md\n+\u2514\u2500 .env.example\n+\n+## 2) Stack\n+\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\n+\n+DB: Postgres 15 with pgvector + pg_trgm.\n+\n+Parsing: Docling (primary), Unstructured (fallback).\n+\n+Bible refs: pythonbible for OSIS normalization.\n+\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\n+\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\n+\n+Frontend: Next.js 14 (App Router), minimal pages.\n+\n+## 3) Ingestion Types (standalone)\n+\n+The engine accepts these source types out of the box. No extension/plugins required.\n+\n+Articles / Papers: .pdf, .docx, .html, .txt\n+\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\n+\n+YouTube: video URL (pull transcript if available; else queue ASR)\n+\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\n+\n+Markdown notes: .md with optional YAML frontmatter\n+\n+Bibliography (optional): CSL-JSON for metadata backfill\n+\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \u00a710).\n+\n+## 4) Infrastructure\n+\n+infra/docker-compose.yml\n+version: \"3.9\"\n+services:\n+  db:\n+    image: postgres:15\n+    environment:\n+      POSTGRES_DB: theo\n+      POSTGRES_PASSWORD: postgres\n+    ports: [\"5432:5432\"]\n+    volumes: [\"db:/var/lib/postgresql/data\"]\n+  redis:\n+    image: redis:7\n+    ports: [\"6379:6379\"]\n+  api:\n+    build: ./services/api\n+    env_file: .env\n+    depends_on: [db, redis]\n+    ports: [\"8000:8000\"]\n+  web:\n+    build: ./services/web\n+    env_file: .env\n+    depends_on: [api]\n+    ports: [\"3000:3000\"]\n+volumes: { db: {} }\n+\n+infra/db-init/pgvector.sql\n+CREATE EXTENSION IF NOT EXISTS vector;\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\n+\n+.env.example\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\n+REDIS_URL=redis://redis:6379/0\n+STORAGE_ROOT=/data\n+EMBEDDING_MODEL=BAAI/bge-m3\n+EMBEDDING_DIM=1024\n+MAX_CHUNK_TOKENS=900\n+DOC_MAX_PAGES=5000\n+USER_AGENT=\"TheoEngine/1.0\"\n+\n+services/api/requirements.txt\n+fastapi[all]==0.115.*\n+uvicorn[standard]==0.30.*\n+psycopg[binary]==3.*\n+SQLAlchemy==2.*\n+pgvector==0.3.*\n+pydantic==2.*\n+python-multipart==0.0.*\n+celery==5.*\n+redis==5.*\n+docling==2.*            # primary parser\n+unstructured==0.15.*# fallback\n+pythonbible==0.1.*      # OSIS normalization\n+regex==2024.*\n+sentence-transformers==3.*\n+flagembedding==1.*# BGE-M3\n+beautifulsoup4==4.*     # web fetch cleanup\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\n+youtube-transcript-api==0.6.*  # transcript fetcher\n+webvtt-py==0.5.*# parse VTT\n+pydub==0.25.*           # audio utils (metadata)\n+\n+## 5) Database Schema (DDL)\n+\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\n+CREATE TABLE documents (\n+  id UUID PRIMARY KEY,\n+  title TEXT,\n+  authors TEXT[],\n+  source_url TEXT,\n+  source_type TEXT CHECK (source_type IN\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\n+  collection TEXT,\n+  pub_date DATE,\n+  channel TEXT,           -- for YouTube/podcasts\n+  video_id TEXT,          -- platform id\n+  duration_seconds INT,   -- if known\n+  bib_json JSONB,\n+  sha256 TEXT UNIQUE,\n+  storage_path TEXT,      -- path to original or normalized pack\n+  created_at TIMESTAMPTZ DEFAULT now()\n+);\n+\n+-- passages: chunked spans with page or time anchors\n+CREATE TABLE passages (\n+  id UUID PRIMARY KEY,\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n+  page_no INT,            -- for paged docs\n+  t_start REAL,           -- seconds (for A/V)\n+  t_end REAL,             -- seconds (for A/V)\n+  start_char INT,\n+  end_char INT,\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\n+  text TEXT NOT NULL,\n+  tokens INT,\n+  embedding vector(1024),\n+  lexeme tsvector,\n+  meta JSONB              -- e.g., speaker, chapter title\n+);\n+\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\n+CREATE INDEX ix_passages_doc ON passages (document_id);\n+\n+## 6) Chunking & Normalization (algorithms)\n+\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\n+\n+6.1 Parsing\n+\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\n+\n+Preserve page numbers and element coordinates when available.\n+\n+6.2 Chunking rules\n+\n+Target ~900 tokens per chunk; clamp to 500\u20131200.\n+\n+Respect block boundaries (headings, paragraphs, list items).\n+\n+Don\u2019t split a detected OSIS span across chunks if avoidable.\n+\n+For PDFs, keep page_no for each chunk.\n+\n+For transcripts:\n+\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\n+\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\n+\n+6.3 OSIS detection\n+\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\n+\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\n+\n+If multiple refs appear, store:\n+\n+osis_ref: minimal covering range,\n+\n+meta.osis_refs_all: full list.\n+\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\u2019s okay; Verse Aggregator dedupes later).\n+\n+## 7) Embeddings & Hybrid Retrieval\n+\n+7.1 Embeddings\n+\n+Model: BAAI/bge-m3 (1024-d). Batch 64\u2013128; L2 normalize.\n+\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\n+\n+7.2 Candidate generation\n+\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\n+\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\n+\n+7.3 Rerank\n+\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\n+\n+Boost passages with matching osis_ref when query includes verses even if text also present.\n+\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\n+\n+## 8) API Contract\n+\n+Document in docs/API.md. Implement in services/api/app/routes/.\n+\n+8.1 Ingest\n+\n+POST /ingest/file \u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\n+Optional frontmatter (JSON). Returns { document_id, status: \"queued\" }.\n+\n+POST /ingest/url \u2014 JSON { url, source_type? }\n+\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\n+\n+If web page: fetch + sanitize \u2192 HTML\u2192text.\n+\n+POST /ingest/transcript \u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\n+\n+POST /jobs/reparse/{document_id} \u2014 enqueue re-ingestion.\n+\n+8.2 Search\n+\n+GET /search\n+Query params: q, osis?, author?, collection?, k?\n+Response:\n+\n+{\n+  \"query\":\"...\",\"results\":[\n+    {\n+      \"document_id\":\"uuid\",\"title\":\"...\",\n+      \"page_no\":12,\"t_start\":123.4,\"t_end\":140.1,\n+      \"osis_ref\":\"John.1.1-5\",\"score\":0.81,\n+      \"snippet\":\"...logos was with God...\"\n+    }\n+  ]\n+}\n+\n+8.3 Verse Aggregator\n+\n+GET /verses/{osis}/mentions\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\n+\n+8.4 Documents\n+\n+GET /documents/{id} \u2014 metadata + list of anchors\n+GET /documents/{id}/passages \u2014 paginated chunks with anchors\n+\n+## 9) Web UI (Next.js 14)\n+\n+/upload \u2014 upload file/URL; show job status.\n+\n+/search \u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\n+\n+PDFs \u2192 ?page={page_no}#passage-{id}\n+\n+A/V \u2192 ?t={t_start}s\n+\n+/verse/[osis] \u2014 Verse Aggregator list of mentions; filters by source type/author.\n+\n+/doc/[id] \u2014 simple reader with passages list and anchors.\n+\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\n+\n+## 10) Frontmatter (optional but supported)\n+\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\n+\n+id: \"uuid-v4\"\n+title: \"Did Jesus Claim to Be God?\"\n+source_type: \"youtube\"         # or \"article\" | \"note\" | \"ai_summary\" ...\n+authors: [\"Ehrman, Bart D.\"]\n+channel: \"Bart D. Ehrman\"\n+video_id: \"abc123\"\n+date: \"2021-03-14\"\n+collection: \"Christology/Debates\"\n+tags: [\"Ehrman\",\"Divinity\"]\n+osis_refs: [\"John.1.1-5\",\"Isa.52.13-53.12\"]   # optional hints\n+sha256: \"content hash\"\n+\n+## 11) Workers & Pipeline (outline code)\n+\n+services/api/app/workers/tasks.py\n+\n+from celery import Celery\n+celery = Celery(__name__, broker=\"redis://redis:6379/0\", backend=\"redis://redis:6379/0\")\n+\n+@celery.task(name=\"tasks.process_file\")\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\n+    # parse -> chunk -> osis -> embed -> upsert\n+\n+@celery.task(name=\"tasks.process_url\")\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\n+\n+services/api/app/ingest/pipeline.py (key steps)\n+\n+def run_pipeline_for_file(doc_id, path, fm):\n+    # 1) detect type by extension; parse (Docling/Unstructured)\n+    # 2) chunk by rules (paged vs transcript)\n+    # 3) detect OSIS -> normalize\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\n+\n+services/api/app/retriever/hybrid.py\n+\n+def search(q: str, osis: str | None, filters: dict, k: int):\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\n+    # 2) dense topK + lexical topK\n+    # 3) rerank & dedupe; return merged list\n+\n+## 12) Definition of Done (MVP)\n+\n+Ingest PDF and YouTube URL successfully \u2192 passages created with correct anchors.\n+\n+/search works for:\n+\n+keyword-only,\n+\n+OSIS-only,\n+\n+combined (keyword + OSIS).\n+\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\n+\n+Web UI pages function (Upload/Search/Verse/Doc).\n+\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\n+\n+## 13) Testing & Fixtures\n+\n+Fixtures:\n+\n+fixtures/pdf/sample_article.pdf \u2014 contains a visible verse citation (e.g., \u201cJohn 1:1\u20135\u201d).\n+\n+fixtures/youtube/transcript.vtt \u2014 with speaker tags and a verse mention.\n+\n+fixtures/markdown/notes.md \u2014 with frontmatter + OSIS refs.\n+\n+Tests:\n+\n+Unit: OSIS regex \u2192 pythonbible normalization (edge cases: ranges, multiple refs).\n+\n+Integration: ingest PDF \u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\n+\n+Verse aggregator: GET /verses/John.1.1/mentions lists \u22651 passage with correct anchors.\n+\n+## 14) Make Targets\n+\n+infra/Makefile\n+\n+up:      ## start all services\n+\\tdocker compose up --build -d\n+down:\n+\\tdocker compose down\n+migrate: ## install extensions\n+\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\n+logs:\n+\\tdocker compose logs -f api web\n+psql:\n+\\tdocker compose exec db psql -U postgres -d theo\n+\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\n+\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\n+\n+Keep hooks:\n+\n+passages.meta for future speaker, chapter, osis_refs_all.\n+\n+Worker queue for passim/CollateX tasks.\n+\n+documents.bib_json for later OpenAlex/GROBID enrichment.\n+\n+## 16) Post-MVP Roadmap (toggle-able)\n+\n+Text-reuse: passim over corpus \u2192 \u201cParallels\u201d sidebar.\n+\n+Alignment: CollateX diff view between selected passages.\n+\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\n+\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\n+\n+IIIF pane: render scanned plates next to normalized text.\n+\n+Auth + collections: user orgs; per-collection indices.\n+\n+## 17) Notes & Guardrails\n+\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\n+\n+Record parser, parser_version, chunker_version in passages.meta.\n+\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\n+\n+Implement a robust range-intersect for OSIS to avoid false misses.\n+\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \u201cupload transcript\u201d path.\n",
    "path": "docs/architecture.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/main",
    "commit": "Add Trufflehog secret scanning baseline and CI workflow (#664)\n\n",
    "commitHash": "22db21b2a4d96b559e89a64780e5f14837a6ce2d",
    "date": "2025-10-12 17:33:31",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/main",
    "commit": "Add Trufflehog secret scanning baseline and CI workflow (#664)\n\n",
    "commitHash": "22db21b2a4d96b559e89a64780e5f14837a6ce2d",
    "date": "2025-10-12 17:33:31",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/add-testcontainer-fixtures-for-postgres-and-pgvector",
    "commit": "Merge branch 'main' into codex/add-testcontainer-fixtures-for-postgres-and-pgvector",
    "commitHash": "d09008730e8a789646a041b7c60c2685627cea29",
    "date": "2025-10-12 17:36:33",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/add-testcontainer-fixtures-for-postgres-and-pgvector",
    "commit": "Merge branch 'main' into codex/add-testcontainer-fixtures-for-postgres-and-pgvector",
    "commitHash": "d09008730e8a789646a041b7c60c2685627cea29",
    "date": "2025-10-12 17:36:33",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/add-testcontainer-fixtures-for-postgres-and-pgvector",
    "commit": "Add property tests and strengthen integration fixtures",
    "commitHash": "6a41c62f2e36b4e379a099b4cd18c3dafe5e74cb",
    "date": "2025-10-12 17:36:05",
    "diff": "@@ -0,0 +1,40 @@\n+# Secret Scanning Strategy\n+\n+## Tool Comparison\n+\n+| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n+| --- | --- | --- |\n+| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n+| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n+| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n+| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n+| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n+\n+**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n+\n+## Baseline Execution\n+\n+1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n+2. Run the filesystem scan from the repo root:\n+   ```bash\n+   python scripts/security/run_trufflehog.py  # or manually:\n+   trufflehog --json --regex --entropy=False --repo_path . file://.\n+   ```\n+3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n+4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n+\n+## False Positive Handling\n+\n+- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n+- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n+- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n+\n+## Continuous Monitoring\n+\n+The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n+\n+## Future Enhancements\n+\n+- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n+- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n+- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/add-testcontainer-fixtures-for-postgres-and-pgvector",
    "commit": "Add property tests and strengthen integration fixtures",
    "commitHash": "6a41c62f2e36b4e379a099b4cd18c3dafe5e74cb",
    "date": "2025-10-12 17:36:05",
    "diff": "@@ -0,0 +1,108 @@\n+[\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n+    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n+    \"date\": \"2025-10-09 23:04:29\",\n+    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n+    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n+    \"date\": \"2025-09-24 13:17:03\",\n+    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"implement ingestion pipeline and hybrid search\",\n+    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n+    \"date\": \"2025-09-23 19:43:10\",\n+    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n+    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n+    \"date\": \"2025-09-23 19:04:10\",\n+    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n+    \"path\": \"docs/architecture.md\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  }\n+]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/compare-trufflehog-and-github-secret-scanning",
    "commit": "Merge branch 'main' into codex/compare-trufflehog-and-github-secret-scanning",
    "commitHash": "6b6ee06028aadcd08d937c70873bbd5ebcb0dba5",
    "date": "2025-10-12 17:33:24",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/compare-trufflehog-and-github-secret-scanning",
    "commit": "Merge branch 'main' into codex/compare-trufflehog-and-github-secret-scanning",
    "commitHash": "6b6ee06028aadcd08d937c70873bbd5ebcb0dba5",
    "date": "2025-10-12 17:33:24",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/compare-trufflehog-and-github-secret-scanning",
    "commit": "Add UI quality gates, accessibility checks, and dashboard (#665)\n\n",
    "commitHash": "8f8e53115bc7b456bdce66879163b4c6670ef868",
    "date": "2025-10-12 17:32:53",
    "diff": "@@ -0,0 +1,40 @@\n+# Secret Scanning Strategy\n+\n+## Tool Comparison\n+\n+| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n+| --- | --- | --- |\n+| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n+| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n+| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n+| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n+| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n+\n+**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n+\n+## Baseline Execution\n+\n+1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n+2. Run the filesystem scan from the repo root:\n+   ```bash\n+   python scripts/security/run_trufflehog.py  # or manually:\n+   trufflehog --json --regex --entropy=False --repo_path . file://.\n+   ```\n+3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n+4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n+\n+## False Positive Handling\n+\n+- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n+- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n+- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n+\n+## Continuous Monitoring\n+\n+The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n+\n+## Future Enhancements\n+\n+- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n+- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n+- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/compare-trufflehog-and-github-secret-scanning",
    "commit": "Add UI quality gates, accessibility checks, and dashboard (#665)\n\n",
    "commitHash": "8f8e53115bc7b456bdce66879163b4c6670ef868",
    "date": "2025-10-12 17:32:53",
    "diff": "@@ -0,0 +1,108 @@\n+[\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n+    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n+    \"date\": \"2025-10-09 23:04:29\",\n+    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n+    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n+    \"date\": \"2025-09-24 13:17:03\",\n+    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"implement ingestion pipeline and hybrid search\",\n+    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n+    \"date\": \"2025-09-23 19:43:10\",\n+    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n+    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n+    \"date\": \"2025-09-23 19:04:10\",\n+    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n+    \"path\": \"docs/architecture.md\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  }\n+]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/compare-trufflehog-and-github-secret-scanning",
    "commit": "Add Trufflehog secret scanning baseline and CI workflow",
    "commitHash": "cb5d6920371f3e29f179a59e440182a4b59ee4e7",
    "date": "2025-10-12 17:32:03",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/compare-trufflehog-and-github-secret-scanning",
    "commit": "Add Trufflehog secret scanning baseline and CI workflow",
    "commitHash": "cb5d6920371f3e29f179a59e440182a4b59ee4e7",
    "date": "2025-10-12 17:32:03",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/embed-pr-template-for-lighthouse-diffs",
    "commit": "feat: expand service manager observability and tooling (#667)\n\n",
    "commitHash": "ab9dc2d86ec00f6b8bae670ce4fb50f1b7196792",
    "date": "2025-10-12 17:35:28",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/embed-pr-template-for-lighthouse-diffs",
    "commit": "feat: expand service manager observability and tooling (#667)\n\n",
    "commitHash": "ab9dc2d86ec00f6b8bae670ce4fb50f1b7196792",
    "date": "2025-10-12 17:35:28",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/embed-pr-template-for-lighthouse-diffs",
    "commit": "Add rag eval automation and performance governance updates",
    "commitHash": "0abe3427da9b831242936f12b8afc80cfa4f016d",
    "date": "2025-10-12 17:34:41",
    "diff": "@@ -0,0 +1,40 @@\n+# Secret Scanning Strategy\n+\n+## Tool Comparison\n+\n+| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n+| --- | --- | --- |\n+| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n+| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n+| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n+| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n+| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n+\n+**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n+\n+## Baseline Execution\n+\n+1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n+2. Run the filesystem scan from the repo root:\n+   ```bash\n+   python scripts/security/run_trufflehog.py  # or manually:\n+   trufflehog --json --regex --entropy=False --repo_path . file://.\n+   ```\n+3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n+4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n+\n+## False Positive Handling\n+\n+- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n+- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n+- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n+\n+## Continuous Monitoring\n+\n+The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n+\n+## Future Enhancements\n+\n+- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n+- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n+- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/embed-pr-template-for-lighthouse-diffs",
    "commit": "Add rag eval automation and performance governance updates",
    "commitHash": "0abe3427da9b831242936f12b8afc80cfa4f016d",
    "date": "2025-10-12 17:34:41",
    "diff": "@@ -0,0 +1,108 @@\n+[\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n+    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n+    \"date\": \"2025-10-09 23:04:29\",\n+    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n+    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n+    \"date\": \"2025-09-24 13:17:03\",\n+    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"implement ingestion pipeline and hybrid search\",\n+    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n+    \"date\": \"2025-09-23 19:43:10\",\n+    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n+    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n+    \"date\": \"2025-09-23 19:04:10\",\n+    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n+    \"path\": \"docs/architecture.md\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  }\n+]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/enhance-deployment-workflow-with-signing-integration",
    "commit": "Add Trufflehog secret scanning baseline and CI workflow (#664)\n\n",
    "commitHash": "22db21b2a4d96b559e89a64780e5f14837a6ce2d",
    "date": "2025-10-12 17:33:31",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/enhance-deployment-workflow-with-signing-integration",
    "commit": "Add Trufflehog secret scanning baseline and CI workflow (#664)\n\n",
    "commitHash": "22db21b2a4d96b559e89a64780e5f14837a6ce2d",
    "date": "2025-10-12 17:33:31",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/enhance-powershell-module-with-health-check-features",
    "commit": "Merge branch 'main' into codex/enhance-powershell-module-with-health-check-features",
    "commitHash": "96e0813fa0d68c8b5186a69ff3590a0246e33643",
    "date": "2025-10-12 17:35:22",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/enhance-powershell-module-with-health-check-features",
    "commit": "Merge branch 'main' into codex/enhance-powershell-module-with-health-check-features",
    "commitHash": "96e0813fa0d68c8b5186a69ff3590a0246e33643",
    "date": "2025-10-12 17:35:22",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/enhance-powershell-module-with-health-check-features",
    "commit": "feat: expand service manager observability and tooling",
    "commitHash": "786319f40fbd2f18949d7177e5f52a3beed04cf3",
    "date": "2025-10-12 17:34:02",
    "diff": "@@ -0,0 +1,40 @@\n+# Secret Scanning Strategy\n+\n+## Tool Comparison\n+\n+| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n+| --- | --- | --- |\n+| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n+| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n+| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n+| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n+| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n+\n+**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n+\n+## Baseline Execution\n+\n+1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n+2. Run the filesystem scan from the repo root:\n+   ```bash\n+   python scripts/security/run_trufflehog.py  # or manually:\n+   trufflehog --json --regex --entropy=False --repo_path . file://.\n+   ```\n+3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n+4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n+\n+## False Positive Handling\n+\n+- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n+- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n+- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n+\n+## Continuous Monitoring\n+\n+The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n+\n+## Future Enhancements\n+\n+- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n+- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n+- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/enhance-powershell-module-with-health-check-features",
    "commit": "feat: expand service manager observability and tooling",
    "commitHash": "786319f40fbd2f18949d7177e5f52a3beed04cf3",
    "date": "2025-10-12 17:34:02",
    "diff": "@@ -0,0 +1,108 @@\n+[\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n+    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n+    \"date\": \"2025-10-09 23:04:29\",\n+    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n+    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n+    \"date\": \"2025-09-24 13:17:03\",\n+    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"implement ingestion pipeline and hybrid search\",\n+    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n+    \"date\": \"2025-09-23 19:43:10\",\n+    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n+    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n+    \"date\": \"2025-09-23 19:04:10\",\n+    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n+    \"path\": \"docs/architecture.md\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  }\n+]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/replace-emoji-icons-with-lucide-components",
    "commit": "Merge branch 'main' into codex/replace-emoji-icons-with-lucide-components",
    "commitHash": "4bf4b015d63e3cff8f1625d5b5cb821e6b9eab66",
    "date": "2025-10-12 17:37:18",
    "diff": "@@ -1,40 +0,0 @@\n-# Secret Scanning Strategy\n-\n-## Tool Comparison\n-\n-| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n-| --- | --- | --- |\n-| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n-| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n-| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n-| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n-| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n-\n-**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n-\n-## Baseline Execution\n-\n-1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n-2. Run the filesystem scan from the repo root:\n-   ```bash\n-   python scripts/security/run_trufflehog.py  # or manually:\n-   trufflehog --json --regex --entropy=False --repo_path . file://.\n-   ```\n-3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n-4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n-\n-## False Positive Handling\n-\n-- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n-- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n-- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n-\n-## Continuous Monitoring\n-\n-The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n-\n-## Future Enhancements\n-\n-- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n-- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n-- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/replace-emoji-icons-with-lucide-components",
    "commit": "Merge branch 'main' into codex/replace-emoji-icons-with-lucide-components",
    "commitHash": "4bf4b015d63e3cff8f1625d5b5cb821e6b9eab66",
    "date": "2025-10-12 17:37:18",
    "diff": "@@ -1,108 +0,0 @@\n-[\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n-    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n-    \"date\": \"2025-10-09 23:04:29\",\n-    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n-    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n-    \"date\": \"2025-09-28 15:47:26\",\n-    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n-    \"path\": \"infra/docker-compose.yml\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n-    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n-    \"date\": \"2025-09-24 13:17:03\",\n-    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"implement ingestion pipeline and hybrid search\",\n-    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n-    \"date\": \"2025-09-23 19:43:10\",\n-    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n-    \"path\": \".env.example\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"scaffold theo engine mvp structure\",\n-    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n-    \"date\": \"2025-09-23 19:17:01\",\n-    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n-    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n-    ]\n-  },\n-  {\n-    \"branch\": \"origin/work\",\n-    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n-    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n-    \"date\": \"2025-09-23 19:04:10\",\n-    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n-    \"path\": \"docs/architecture.md\",\n-    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n-    \"reason\": \"Password in URL\",\n-    \"stringsFound\": [\n-      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n-    ]\n-  }\n-]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  },
  {
    "branch": "origin/codex/replace-emoji-icons-with-lucide-components",
    "commit": "Improve icons, error handling, and loading states",
    "commitHash": "8ab42314547bd96849e5e7aa8d56769d2947e605",
    "date": "2025-10-12 17:37:00",
    "diff": "@@ -0,0 +1,40 @@\n+# Secret Scanning Strategy\n+\n+## Tool Comparison\n+\n+| Capability | Trufflehog OSS (v2/v3 CLI) | GitHub Advanced Security Secret Scanning |\n+| --- | --- | --- |\n+| **Coverage** | Scans local clones, git history, file system paths, Docker/Cloud targets. Works offline and can be run pre-commit or in any CI. | Scans pushes to GitHub repositories (branches, tags, and historical git objects). No local/offline scanning; relies on GitHub SaaS. |\n+| **Rules & Tuning** | Custom detectors via regex and entropy, allow-list files, baselines, and targeted path filters. Supports curated baselines in-repo (see [`trufflehog-baseline.json`](trufflehog-baseline.json)). | Built-in ~200 provider patterns + custom patterns defined in `.github/secret-scanning.yml`. False-positive suppression managed through GitHub UI or commit/message allow-lists. |\n+| **Footprint & Dependencies** | Lightweight Python/Go binary; no external services. Fits repo's mixed Python/TypeScript stack without additional runtime. Local pilots possible for contributors without GitHub Enterprise. | Requires GitHub Advanced Security license; scanning occurs server-side which can delay detection for local-only experiments. Cannot be executed within our existing local `scripts/` automation. |\n+| **Compliance Signals** | Generates JSON artifacts for audit evidence (SOC 2, ISO 27001) that can be stored alongside CI logs. Supports scheduled scans to demonstrate continuous monitoring. | Native integration with GitHub Security Center simplifies evidencing, but relies on GitHub retention (90 days). Exporting artifacts for external audits requires API access. |\n+| **Alert Routing** | CI workflow can fail builds, upload artifacts, and page on-call via existing incident tooling. | Alerts appear in GitHub Security tab; need additional automation/webhooks to page incident responders. |\n+\n+**Decision:** Trufflehog OSS remains the primary scanner. It runs locally, surfaces the default-credential templates that exist in this repo, and can be extended via configuration files stored here. GitHub Advanced Security remains optional; enabling it later would provide a secondary control but requires license enablement outside of this code change.\n+\n+## Baseline Execution\n+\n+1. Install Trufflehog locally (`pip install trufflehog`) and ensure the repository has a remote named `origin` (needed by legacy CLI).\n+2. Run the filesystem scan from the repo root:\n+   ```bash\n+   python scripts/security/run_trufflehog.py  # or manually:\n+   trufflehog --json --regex --entropy=False --repo_path . file://.\n+   ```\n+3. The JSON findings are persisted to [`docs/security/trufflehog-baseline.json`](trufflehog-baseline.json). Eight findings were recorded on 2025-01-13; all map to sample Postgres/Redis connection strings used in local development templates and infrastructure manifests.\n+4. Treat the baseline as the allow-list. Confirm any new match is not in that file before updating the baseline.\n+\n+## False Positive Handling\n+\n+- **Expected credentials:** Postgres DSNs (`postgresql+psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, `infra/docker-compose.yml`, and `theo/infrastructure/api/app/core/settings.py` to document local defaults. They are not production secrets.\n+- **Suppressing noise:** After verifying a finding is a non-sensitive template value, add or update its record in `trufflehog-baseline.json` with the latest commit hash. Baseline entries store the commit path, reason, and string snippet so auditors can trace the exception.\n+- **Escalation:** If a finding references any credential outside of the documented templates, treat it as a potential leak and follow the remediation steps in [`SECURITY.md`](../../SECURITY.md).\n+\n+## Continuous Monitoring\n+\n+The new GitHub Actions workflow (`.github/workflows/secret-scanning.yml`) executes Trufflehog on every push, pull request, and a weekly scheduled run. The job compares live findings against the baseline; the build fails and uploads an artifact if a new secret is detected.\n+\n+## Future Enhancements\n+\n+- Enable GitHub Advanced Security when licensing is available to gain cross-repo correlation and integration with GitHub's security dashboard.\n+- Replace the legacy Python CLI with the Go binary to remove the `origin` remote requirement and improve performance once the CI migration is validated.\n+- Extend the baseline management script to auto-open issues when a new secret appears instead of only failing CI.\n",
    "path": "docs/security/secret-scanning.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, \u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo`) and Redis URLs (`redis://redis:6379/0`) appear in `.env.example`, "
    ]
  },
  {
    "branch": "origin/codex/replace-emoji-icons-with-lucide-components",
    "commit": "Improve icons, error handling, and loading states",
    "commitHash": "8ab42314547bd96849e5e7aa8d56769d2947e605",
    "date": "2025-10-12 17:37:00",
    "diff": "@@ -0,0 +1,108 @@\n+[\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: add Model Context Protocol (MCP) server with Docker and dev script support\\n\",\n+    \"commitHash\": \"5d5f3ba8208ab1305dd1dddc654564895f440422\",\n+    \"date\": \"2025-10-09 23:04:29\",\n+    \"diff\": \"@@ -47,28 +47,6 @@ services:\\n     volumes:\\n       - storage:/data/storage\\n \\n-  mcp:\\n-    build:\\n-      context: ..\\n-      dockerfile: mcp_server/Dockerfile\\n-    env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      MCP_HOST: 0.0.0.0\\n-      MCP_PORT: 8050\\n-    depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n-    ports:\\n-      - \\\"8050:8050\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n     build:\\n       context: ..\\n@@ -85,4 +63,4 @@ services:\\n \\n volumes:\\n   db: {}\\n-  storage: {}\\n+  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,11 +1,13 @@\\n # TheoEngine environment variables (example)\\n-# Copy to .env and adjust for your environment.\\n+# Copy to .env and adjust.\\n # pwsh: Copy-Item .env.example .env -Force\\n \\n # --- API (FastAPI) ---\\n-# Local development defaults (SQLite + local storage)\\n+# Local dev: SQLite + local storage directory\\n database_url=sqlite:///./theo.db\\n storage_root=./storage\\n+\\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n redis_url=redis://localhost:6379/0\\n \\n # Ingestion/embedding defaults\\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\\n # Optional fixtures path override (auto-detected if ./fixtures exists)\\n # fixtures_root=./fixtures\\n \\n-# --- API authentication toggles (optional) ---\\n-# THEO_API_KEYS=alpha,beta\\n-# THEO_AUTH_JWT_SECRET=change-me\\n-# THEO_AUTH_JWT_AUDIENCE=theo\\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\\n-\\n # --- Web (Next.js) ---\\n # Point the UI to the API in local dev\\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n API_BASE_URL=http://127.0.0.1:8000\\n \\n-# --- Docker Compose ---\\n-# docker compose (in ./infra) reads this same file and overrides the core\\n-# connection settings internally. Leave these defaults in place unless you\\n-# are running the database or broker elsewhere.\\n-# During compose runs the api service sets:\\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web service sets API urls to http://api:8000.\\n+# --- Docker Compose variants (uncomment if using compose) ---\\n+# Inside the compose network, reference services by name\\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n+# API_BASE_URL=http://api:8000\\n+\\n+# For API using Postgres and Redis services in compose\\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a data volume\\n+# storage_root=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"feat: enhance Docker setup, improve API routes, and update environment configurations\\n\",\n+    \"commitHash\": \"d547bd31ca798ef6f1637c5527a7b79343d77543\",\n+    \"date\": \"2025-09-28 15:47:26\",\n+    \"diff\": \"@@ -1,66 +1,32 @@\\n version: \\\"3.9\\\"\\n-\\n services:\\n   db:\\n     image: postgres:15\\n     environment:\\n       POSTGRES_DB: theo\\n-      POSTGRES_USER: postgres\\n       POSTGRES_PASSWORD: postgres\\n     ports:\\n       - \\\"5432:5432\\\"\\n     volumes:\\n       - db:/var/lib/postgresql/data\\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\\n-    healthcheck:\\n-      test: [\\\"CMD-SHELL\\\", \\\"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   redis:\\n     image: redis:7\\n     ports:\\n       - \\\"6379:6379\\\"\\n-    healthcheck:\\n-      test: [\\\"CMD\\\", \\\"redis-cli\\\", \\\"PING\\\"]\\n-      interval: 10s\\n-      timeout: 5s\\n-      retries: 5\\n-\\n   api:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/api/Dockerfile\\n+    build: ../services/api\\n     env_file: ../.env\\n-    environment:\\n-      storage_root: /data/storage\\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        condition: service_healthy\\n-      redis:\\n-        condition: service_healthy\\n+      - db\\n+      - redis\\n     ports:\\n       - \\\"8000:8000\\\"\\n-    volumes:\\n-      - storage:/data/storage\\n-\\n   web:\\n-    build:\\n-      context: ..\\n-      dockerfile: theo/services/web/Dockerfile\\n+    build: ../services/web\\n     env_file: ../.env\\n-    environment:\\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\\n-      API_BASE_URL: http://api:8000\\n     depends_on:\\n-      api:\\n-        condition: service_started\\n+      - api\\n     ports:\\n       - \\\"3000:3000\\\"\\n-\\n volumes:\\n   db: {}\\n-  storage: {}\\n\\\\ No newline at end of file\\n\",\n+    \"path\": \"infra/docker-compose.yml\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Refactor example environment variables for clarity and local development setup\\n\",\n+    \"commitHash\": \"93cc7b51d2e05d903f2d4f28544224a25ee7fe79\",\n+    \"date\": \"2025-09-24 13:17:03\",\n+    \"diff\": \"@@ -1,39 +1,3 @@\\n-# TheoEngine environment variables (example)\\n-# Copy to .env and adjust.\\n-# pwsh: Copy-Item .env.example .env -Force\\n-\\n-# --- API (FastAPI) ---\\n-# Local dev: SQLite + local storage directory\\n-database_url=sqlite:///./theo.db\\n-storage_root=./storage\\n-\\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\\n-redis_url=redis://localhost:6379/0\\n-\\n-# Ingestion/embedding defaults\\n-embedding_model=BAAI/bge-m3\\n-embedding_dim=1024\\n-max_chunk_tokens=900\\n-doc_max_pages=5000\\n-transcript_max_window=40.0\\n-user_agent=TheoEngine/1.0\\n-\\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\\n-# fixtures_root=./fixtures\\n-\\n-# --- Web (Next.js) ---\\n-# Point the UI to the API in local dev\\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\\n-API_BASE_URL=http://127.0.0.1:8000\\n-\\n-# --- Docker Compose variants (uncomment if using compose) ---\\n-# Inside the compose network, reference services by name\\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\\n-# API_BASE_URL=http://api:8000\\n-\\n-# For API using Postgres and Redis services in compose\\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a data volume\\n-# storage_root=/data/storage\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\",\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"implement ingestion pipeline and hybrid search\",\n+    \"commitHash\": \"8d1ab58da8e55381b94425a7dffc86d6a9c75437\",\n+    \"date\": \"2025-09-23 19:43:10\",\n+    \"diff\": \"@@ -1,35 +1,12 @@\\n-\\\"\\\"\\\"Application configuration for the Theo Engine API.\\\"\\\"\\\"\\n+\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n \\n-from functools import lru_cache\\n-from pathlib import Path\\n-\\n-from pydantic import Field\\n-from pydantic_settings import BaseSettings, SettingsConfigDict\\n+from pydantic import BaseSettings, Field\\n \\n \\n class Settings(BaseSettings):\\n-    \\\"\\\"\\\"Runtime configuration loaded from environment variables.\\\"\\\"\\\"\\n-\\n-    model_config = SettingsConfigDict(env_prefix=\\\"\\\", env_file=\\\".env\\\", env_file_encoding=\\\"utf-8\\\")\\n-\\n-    database_url: str = Field(\\n-        default=\\\"sqlite:///./theo.db\\\", description=\\\"SQLAlchemy database URL\\\"\\n-    )\\n-    redis_url: str = Field(default=\\\"redis://redis:6379/0\\\", description=\\\"Celery broker URL\\\")\\n-    storage_root: Path = Field(default=Path(\\\"./storage\\\"), description=\\\"Location for persisted artifacts\\\")\\n-    embedding_model: str = Field(default=\\\"BAAI/bge-m3\\\")\\n-    embedding_dim: int = Field(default=1024)\\n-    max_chunk_tokens: int = Field(default=900)\\n-    doc_max_pages: int = Field(default=5000)\\n-    user_agent: str = Field(default=\\\"TheoEngine/1.0\\\")\\n-\\n-@lru_cache\\n-def get_settings() -> Settings:\\n-    \\\"\\\"\\\"Return a cached Settings instance.\\\"\\\"\\\"\\n-\\n-    settings = Settings()\\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\\n-    return settings\\n+    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n \\n \\n-settings = get_settings()\\n+settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,3 +0,0 @@\\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n+    \"path\": \".env.example\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"scaffold theo engine mvp structure\",\n+    \"commitHash\": \"6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910\",\n+    \"date\": \"2025-09-23 19:17:01\",\n+    \"diff\": \"@@ -1,12 +0,0 @@\\n-\\\"\\\"\\\"Application configuration.\\\"\\\"\\\"\\n-\\n-from pydantic import BaseSettings, Field\\n-\\n-\\n-class Settings(BaseSettings):\\n-    database_url: str = Field(env=\\\"DATABASE_URL\\\", default=\\\"postgresql+psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n-    storage_root: str = Field(env=\\\"STORAGE_ROOT\\\", default=\\\"/data/storage\\\")\\n-\\n-\\n-settings = Settings()\\n\",\n+    \"path\": \"theo/infrastructure/api/app/core/settings.py\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n+    ]\n+  },\n+  {\n+    \"branch\": \"origin/work\",\n+    \"commit\": \"Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\\n\",\n+    \"commitHash\": \"e78da1f51a29708b06c70056b0485588372e368f\",\n+    \"date\": \"2025-09-23 19:04:10\",\n+    \"diff\": \"@@ -0,0 +1,458 @@\\n+# Theo Engine \\u2014 Final Build Spec (Standalone)\\n+\\n+## 0) Mission & MVP\\n+\\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\\n+\\n+MVP outcomes (no LLM required):\\n+\\n+Ingest local files and URLs (including YouTube).\\n+\\n+Parse to chunked, citation-preserving passages with page/time anchors.\\n+\\n+Detect and normalize Bible references \\u2192 OSIS.\\n+\\n+Hybrid search (pgvector embeddings + lexical).\\n+\\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \\u2192 see every snippet across the corpus, with jump links (page/time).\\n+\\n+Minimal web UI: Upload, Search, Verse, Document.\\n+\\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\\n+\\n+## 1) Repo Layout (monorepo)\\n+\\n+theo/\\n+\\u251c\\u2500 services/\\n+\\u2502  \\u251c\\u2500 api/                 # FastAPI service\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 main.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 routes/        # FastAPI routers\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 search.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u251c\\u2500 verses.py\\n+\\u2502  \\u2502  \\u2502  \\u2502  \\u2514\\u2500 documents.py\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 core/          # db, settings, logging\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 ingest/        # parsers, chunkers, osis detection\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 retriever/     # hybrid search/rerank\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 models/        # pydantic schemas\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 workers/       # Celery tasks\\n+\\u2502  \\u2502  \\u2514\\u2500 requirements.txt\\n+\\u2502  \\u251c\\u2500 web/                 # Next.js 14 (App Router)\\n+\\u2502  \\u2502  \\u251c\\u2500 app/\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 upload/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 search/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u251c\\u2500 verse/[osis]/page.tsx\\n+\\u2502  \\u2502  \\u2502  \\u2514\\u2500 doc/[id]/page.tsx\\n+\\u2502  \\u2502  \\u2514\\u2500 package.json\\n+\\u2502  \\u2514\\u2500 cli/                 # optional: bulk ingest CLI\\n+\\u2502     \\u2514\\u2500 ingest_folder.py\\n+\\u251c\\u2500 infra/\\n+\\u2502  \\u251c\\u2500 docker-compose.yml\\n+\\u2502  \\u251c\\u2500 db-init/pgvector.sql\\n+\\u2502  \\u2514\\u2500 Makefile\\n+\\u251c\\u2500 docs/\\n+\\u2502  \\u251c\\u2500 API.md\\n+\\u2502  \\u251c\\u2500 Chunking.md\\n+\\u2502  \\u251c\\u2500 OSIS.md\\n+\\u2502  \\u2514\\u2500 Frontmatter.md\\n+\\u2514\\u2500 .env.example\\n+\\n+## 2) Stack\\n+\\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\\n+\\n+DB: Postgres 15 with pgvector + pg_trgm.\\n+\\n+Parsing: Docling (primary), Unstructured (fallback).\\n+\\n+Bible refs: pythonbible for OSIS normalization.\\n+\\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\\n+\\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\\n+\\n+Frontend: Next.js 14 (App Router), minimal pages.\\n+\\n+## 3) Ingestion Types (standalone)\\n+\\n+The engine accepts these source types out of the box. No extension/plugins required.\\n+\\n+Articles / Papers: .pdf, .docx, .html, .txt\\n+\\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\\n+\\n+YouTube: video URL (pull transcript if available; else queue ASR)\\n+\\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\\n+\\n+Markdown notes: .md with optional YAML frontmatter\\n+\\n+Bibliography (optional): CSL-JSON for metadata backfill\\n+\\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \\u00a710).\\n+\\n+## 4) Infrastructure\\n+\\n+infra/docker-compose.yml\\n+version: \\\"3.9\\\"\\n+services:\\n+  db:\\n+    image: postgres:15\\n+    environment:\\n+      POSTGRES_DB: theo\\n+      POSTGRES_PASSWORD: postgres\\n+    ports: [\\\"5432:5432\\\"]\\n+    volumes: [\\\"db:/var/lib/postgresql/data\\\"]\\n+  redis:\\n+    image: redis:7\\n+    ports: [\\\"6379:6379\\\"]\\n+  api:\\n+    build: ./services/api\\n+    env_file: .env\\n+    depends_on: [db, redis]\\n+    ports: [\\\"8000:8000\\\"]\\n+  web:\\n+    build: ./services/web\\n+    env_file: .env\\n+    depends_on: [api]\\n+    ports: [\\\"3000:3000\\\"]\\n+volumes: { db: {} }\\n+\\n+infra/db-init/pgvector.sql\\n+CREATE EXTENSION IF NOT EXISTS vector;\\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\\n+\\n+.env.example\\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data\\n+EMBEDDING_MODEL=BAAI/bge-m3\\n+EMBEDDING_DIM=1024\\n+MAX_CHUNK_TOKENS=900\\n+DOC_MAX_PAGES=5000\\n+USER_AGENT=\\\"TheoEngine/1.0\\\"\\n+\\n+services/api/requirements.txt\\n+fastapi[all]==0.115.*\\n+uvicorn[standard]==0.30.*\\n+psycopg[binary]==3.*\\n+SQLAlchemy==2.*\\n+pgvector==0.3.*\\n+pydantic==2.*\\n+python-multipart==0.0.*\\n+celery==5.*\\n+redis==5.*\\n+docling==2.*            # primary parser\\n+unstructured==0.15.*# fallback\\n+pythonbible==0.1.*      # OSIS normalization\\n+regex==2024.*\\n+sentence-transformers==3.*\\n+flagembedding==1.*# BGE-M3\\n+beautifulsoup4==4.*     # web fetch cleanup\\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\\n+youtube-transcript-api==0.6.*  # transcript fetcher\\n+webvtt-py==0.5.*# parse VTT\\n+pydub==0.25.*           # audio utils (metadata)\\n+\\n+## 5) Database Schema (DDL)\\n+\\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\\n+CREATE TABLE documents (\\n+  id UUID PRIMARY KEY,\\n+  title TEXT,\\n+  authors TEXT[],\\n+  source_url TEXT,\\n+  source_type TEXT CHECK (source_type IN\\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\\n+  collection TEXT,\\n+  pub_date DATE,\\n+  channel TEXT,           -- for YouTube/podcasts\\n+  video_id TEXT,          -- platform id\\n+  duration_seconds INT,   -- if known\\n+  bib_json JSONB,\\n+  sha256 TEXT UNIQUE,\\n+  storage_path TEXT,      -- path to original or normalized pack\\n+  created_at TIMESTAMPTZ DEFAULT now()\\n+);\\n+\\n+-- passages: chunked spans with page or time anchors\\n+CREATE TABLE passages (\\n+  id UUID PRIMARY KEY,\\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\\n+  page_no INT,            -- for paged docs\\n+  t_start REAL,           -- seconds (for A/V)\\n+  t_end REAL,             -- seconds (for A/V)\\n+  start_char INT,\\n+  end_char INT,\\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\\n+  text TEXT NOT NULL,\\n+  tokens INT,\\n+  embedding vector(1024),\\n+  lexeme tsvector,\\n+  meta JSONB              -- e.g., speaker, chapter title\\n+);\\n+\\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\\n+CREATE INDEX ix_passages_doc ON passages (document_id);\\n+\\n+## 6) Chunking & Normalization (algorithms)\\n+\\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\\n+\\n+6.1 Parsing\\n+\\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\\n+\\n+Preserve page numbers and element coordinates when available.\\n+\\n+6.2 Chunking rules\\n+\\n+Target ~900 tokens per chunk; clamp to 500\\u20131200.\\n+\\n+Respect block boundaries (headings, paragraphs, list items).\\n+\\n+Don\\u2019t split a detected OSIS span across chunks if avoidable.\\n+\\n+For PDFs, keep page_no for each chunk.\\n+\\n+For transcripts:\\n+\\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\\n+\\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\\n+\\n+6.3 OSIS detection\\n+\\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\\n+\\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\\n+\\n+If multiple refs appear, store:\\n+\\n+osis_ref: minimal covering range,\\n+\\n+meta.osis_refs_all: full list.\\n+\\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\\u2019s okay; Verse Aggregator dedupes later).\\n+\\n+## 7) Embeddings & Hybrid Retrieval\\n+\\n+7.1 Embeddings\\n+\\n+Model: BAAI/bge-m3 (1024-d). Batch 64\\u2013128; L2 normalize.\\n+\\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\\n+\\n+7.2 Candidate generation\\n+\\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\\n+\\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\\n+\\n+7.3 Rerank\\n+\\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\\n+\\n+Boost passages with matching osis_ref when query includes verses even if text also present.\\n+\\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\\n+\\n+## 8) API Contract\\n+\\n+Document in docs/API.md. Implement in services/api/app/routes/.\\n+\\n+8.1 Ingest\\n+\\n+POST /ingest/file \\u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\\n+Optional frontmatter (JSON). Returns { document_id, status: \\\"queued\\\" }.\\n+\\n+POST /ingest/url \\u2014 JSON { url, source_type? }\\n+\\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\\n+\\n+If web page: fetch + sanitize \\u2192 HTML\\u2192text.\\n+\\n+POST /ingest/transcript \\u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\\n+\\n+POST /jobs/reparse/{document_id} \\u2014 enqueue re-ingestion.\\n+\\n+8.2 Search\\n+\\n+GET /search\\n+Query params: q, osis?, author?, collection?, k?\\n+Response:\\n+\\n+{\\n+  \\\"query\\\":\\\"...\\\",\\\"results\\\":[\\n+    {\\n+      \\\"document_id\\\":\\\"uuid\\\",\\\"title\\\":\\\"...\\\",\\n+      \\\"page_no\\\":12,\\\"t_start\\\":123.4,\\\"t_end\\\":140.1,\\n+      \\\"osis_ref\\\":\\\"John.1.1-5\\\",\\\"score\\\":0.81,\\n+      \\\"snippet\\\":\\\"...logos was with God...\\\"\\n+    }\\n+  ]\\n+}\\n+\\n+8.3 Verse Aggregator\\n+\\n+GET /verses/{osis}/mentions\\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\\n+\\n+8.4 Documents\\n+\\n+GET /documents/{id} \\u2014 metadata + list of anchors\\n+GET /documents/{id}/passages \\u2014 paginated chunks with anchors\\n+\\n+## 9) Web UI (Next.js 14)\\n+\\n+/upload \\u2014 upload file/URL; show job status.\\n+\\n+/search \\u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\\n+\\n+PDFs \\u2192 ?page={page_no}#passage-{id}\\n+\\n+A/V \\u2192 ?t={t_start}s\\n+\\n+/verse/[osis] \\u2014 Verse Aggregator list of mentions; filters by source type/author.\\n+\\n+/doc/[id] \\u2014 simple reader with passages list and anchors.\\n+\\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\\n+\\n+## 10) Frontmatter (optional but supported)\\n+\\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\\n+\\n+id: \\\"uuid-v4\\\"\\n+title: \\\"Did Jesus Claim to Be God?\\\"\\n+source_type: \\\"youtube\\\"         # or \\\"article\\\" | \\\"note\\\" | \\\"ai_summary\\\" ...\\n+authors: [\\\"Ehrman, Bart D.\\\"]\\n+channel: \\\"Bart D. Ehrman\\\"\\n+video_id: \\\"abc123\\\"\\n+date: \\\"2021-03-14\\\"\\n+collection: \\\"Christology/Debates\\\"\\n+tags: [\\\"Ehrman\\\",\\\"Divinity\\\"]\\n+osis_refs: [\\\"John.1.1-5\\\",\\\"Isa.52.13-53.12\\\"]   # optional hints\\n+sha256: \\\"content hash\\\"\\n+\\n+## 11) Workers & Pipeline (outline code)\\n+\\n+services/api/app/workers/tasks.py\\n+\\n+from celery import Celery\\n+celery = Celery(__name__, broker=\\\"redis://redis:6379/0\\\", backend=\\\"redis://redis:6379/0\\\")\\n+\\n+@celery.task(name=\\\"tasks.process_file\\\")\\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\\n+    # parse -> chunk -> osis -> embed -> upsert\\n+\\n+@celery.task(name=\\\"tasks.process_url\\\")\\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\\n+\\n+services/api/app/ingest/pipeline.py (key steps)\\n+\\n+def run_pipeline_for_file(doc_id, path, fm):\\n+    # 1) detect type by extension; parse (Docling/Unstructured)\\n+    # 2) chunk by rules (paged vs transcript)\\n+    # 3) detect OSIS -> normalize\\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\\n+\\n+services/api/app/retriever/hybrid.py\\n+\\n+def search(q: str, osis: str | None, filters: dict, k: int):\\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\\n+    # 2) dense topK + lexical topK\\n+    # 3) rerank & dedupe; return merged list\\n+\\n+## 12) Definition of Done (MVP)\\n+\\n+Ingest PDF and YouTube URL successfully \\u2192 passages created with correct anchors.\\n+\\n+/search works for:\\n+\\n+keyword-only,\\n+\\n+OSIS-only,\\n+\\n+combined (keyword + OSIS).\\n+\\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\\n+\\n+Web UI pages function (Upload/Search/Verse/Doc).\\n+\\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\\n+\\n+## 13) Testing & Fixtures\\n+\\n+Fixtures:\\n+\\n+fixtures/pdf/sample_article.pdf \\u2014 contains a visible verse citation (e.g., \\u201cJohn 1:1\\u20135\\u201d).\\n+\\n+fixtures/youtube/transcript.vtt \\u2014 with speaker tags and a verse mention.\\n+\\n+fixtures/markdown/notes.md \\u2014 with frontmatter + OSIS refs.\\n+\\n+Tests:\\n+\\n+Unit: OSIS regex \\u2192 pythonbible normalization (edge cases: ranges, multiple refs).\\n+\\n+Integration: ingest PDF \\u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\\n+\\n+Verse aggregator: GET /verses/John.1.1/mentions lists \\u22651 passage with correct anchors.\\n+\\n+## 14) Make Targets\\n+\\n+infra/Makefile\\n+\\n+up:      ## start all services\\n+\\\\tdocker compose up --build -d\\n+down:\\n+\\\\tdocker compose down\\n+migrate: ## install extensions\\n+\\\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\\n+logs:\\n+\\\\tdocker compose logs -f api web\\n+psql:\\n+\\\\tdocker compose exec db psql -U postgres -d theo\\n+\\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\\n+\\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\\n+\\n+Keep hooks:\\n+\\n+passages.meta for future speaker, chapter, osis_refs_all.\\n+\\n+Worker queue for passim/CollateX tasks.\\n+\\n+documents.bib_json for later OpenAlex/GROBID enrichment.\\n+\\n+## 16) Post-MVP Roadmap (toggle-able)\\n+\\n+Text-reuse: passim over corpus \\u2192 \\u201cParallels\\u201d sidebar.\\n+\\n+Alignment: CollateX diff view between selected passages.\\n+\\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\\n+\\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\\n+\\n+IIIF pane: render scanned plates next to normalized text.\\n+\\n+Auth + collections: user orgs; per-collection indices.\\n+\\n+## 17) Notes & Guardrails\\n+\\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\\n+\\n+Record parser, parser_version, chunker_version in passages.meta.\\n+\\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\\n+\\n+Implement a robust range-intersect for OSIS to avoid false misses.\\n+\\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \\u201cupload transcript\\u201d path.\\n\",\n+    \"path\": \"docs/architecture.md\",\n+    \"printDiff\": \"\\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n+    \"reason\": \"Password in URL\",\n+    \"stringsFound\": [\n+      \"psycopg://postgres:postgres@db:5432/theo\\n\"\n+    ]\n+  }\n+]\n\\ No newline at end of file\n",
    "path": "docs/security/trufflehog-baseline.json",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\\n\"\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n-      MCP_TOOLS_ENABLED: \\\"1\\\"\\n-      ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-#   redis_url=redis://redis:6379/0\\n-#   storage_root=/data/storage\\n-# and the web ",
      "psycopg://postgres:postgres@db:5432/theo\\n+# redis_url=redis://redis:6379/0\\n+\\n+# If running API in a container and mounting a ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-      redis_url: redis://redis:6379/0\\n     depends_on:\\n-      db:\\n-        ",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-# redis_url=redis://redis:6379/0\\n-\\n-# If running API in a container and mounting a ",
      "psycopg://postgres:postgres@db:5432/theo\\n+REDIS_URL=redis://redis:6379/0\\n+STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n+    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n+ ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\n-REDIS_URL=redis://redis:6379/0\\n-STORAGE_ROOT=/data/storage\\n\",\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n-    redis_url: str = Field(env=\\\"REDIS_URL\\\", default=\\\"redis://redis:6379/0\\\")\\n- ",
      "mpsycopg://postgres:postgres@db:5432/theo\\\")\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\\")\\n\"\n",
      "mpsycopg://postgres:postgres@db:5432/theo\\n\\u001b[0m\",\n",
      "psycopg://postgres:postgres@db:5432/theo\\n\"\n"
    ]
  }
]
