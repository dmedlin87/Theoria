[
  {
    "branch": "origin/work",
    "commit": "feat: add Model Context Protocol (MCP) server with Docker and dev script support\n",
    "commitHash": "5d5f3ba8208ab1305dd1dddc654564895f440422",
    "date": "2025-10-09 23:04:29",
    "diff": "@@ -47,28 +47,6 @@ services:\n     volumes:\n       - storage:/data/storage\n \n-  mcp:\n-    build:\n-      context: ..\n-      dockerfile: mcp_server/Dockerfile\n-    env_file: ../.env\n-    environment:\n-      storage_root: /data/storage\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\n-      redis_url: redis://redis:6379/0\n-      MCP_TOOLS_ENABLED: \"1\"\n-      MCP_HOST: 0.0.0.0\n-      MCP_PORT: 8050\n-    depends_on:\n-      db:\n-        condition: service_healthy\n-      redis:\n-        condition: service_healthy\n-    ports:\n-      - \"8050:8050\"\n-    volumes:\n-      - storage:/data/storage\n-\n   web:\n     build:\n       context: ..\n@@ -85,4 +63,4 @@ services:\n \n volumes:\n   db: {}\n-  storage: {}\n+  storage: {}\n\\ No newline at end of file\n",
    "path": "infra/docker-compose.yml",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "feat: enhance Docker setup, improve API routes, and update environment configurations\n",
    "commitHash": "d547bd31ca798ef6f1637c5527a7b79343d77543",
    "date": "2025-09-28 15:47:26",
    "diff": "@@ -1,11 +1,13 @@\n # TheoEngine environment variables (example)\n-# Copy to .env and adjust for your environment.\n+# Copy to .env and adjust.\n # pwsh: Copy-Item .env.example .env -Force\n \n # --- API (FastAPI) ---\n-# Local development defaults (SQLite + local storage)\n+# Local dev: SQLite + local storage directory\n database_url=sqlite:///./theo.db\n storage_root=./storage\n+\n+# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\n redis_url=redis://localhost:6379/0\n \n # Ingestion/embedding defaults\n@@ -19,23 +21,19 @@ user_agent=TheoEngine/1.0\n # Optional fixtures path override (auto-detected if ./fixtures exists)\n # fixtures_root=./fixtures\n \n-# --- API authentication toggles (optional) ---\n-# THEO_API_KEYS=alpha,beta\n-# THEO_AUTH_JWT_SECRET=change-me\n-# THEO_AUTH_JWT_AUDIENCE=theo\n-# THEO_AUTH_JWT_ISSUER=https://example.com/auth\n-\n # --- Web (Next.js) ---\n # Point the UI to the API in local dev\n NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\n API_BASE_URL=http://127.0.0.1:8000\n \n-# --- Docker Compose ---\n-# docker compose (in ./infra) reads this same file and overrides the core\n-# connection settings internally. Leave these defaults in place unless you\n-# are running the database or broker elsewhere.\n-# During compose runs the api service sets:\n-#   database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\n-#   redis_url=redis://redis:6379/0\n-#   storage_root=/data/storage\n-# and the web service sets API urls to http://api:8000.\n+# --- Docker Compose variants (uncomment if using compose) ---\n+# Inside the compose network, reference services by name\n+# NEXT_PUBLIC_API_BASE_URL=http://api:8000\n+# API_BASE_URL=http://api:8000\n+\n+# For API using Postgres and Redis services in compose\n+# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\n+# redis_url=redis://redis:6379/0\n+\n+# If running API in a container and mounting a data volume\n+# storage_root=/data/storage\n",
    "path": ".env.example",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n",
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "feat: enhance Docker setup, improve API routes, and update environment configurations\n",
    "commitHash": "d547bd31ca798ef6f1637c5527a7b79343d77543",
    "date": "2025-09-28 15:47:26",
    "diff": "@@ -1,66 +1,32 @@\n version: \"3.9\"\n-\n services:\n   db:\n     image: postgres:15\n     environment:\n       POSTGRES_DB: theo\n-      POSTGRES_USER: postgres\n       POSTGRES_PASSWORD: postgres\n     ports:\n       - \"5432:5432\"\n     volumes:\n       - db:/var/lib/postgresql/data\n-      - ./db-init:/docker-entrypoint-initdb.d:ro\n-    healthcheck:\n-      test: [\"CMD-SHELL\", \"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\"]\n-      interval: 10s\n-      timeout: 5s\n-      retries: 5\n-\n   redis:\n     image: redis:7\n     ports:\n       - \"6379:6379\"\n-    healthcheck:\n-      test: [\"CMD\", \"redis-cli\", \"PING\"]\n-      interval: 10s\n-      timeout: 5s\n-      retries: 5\n-\n   api:\n-    build:\n-      context: ..\n-      dockerfile: theo/services/api/Dockerfile\n+    build: ../services/api\n     env_file: ../.env\n-    environment:\n-      storage_root: /data/storage\n-      database_url: postgresql+psycopg://postgres:postgres@db:5432/theo\n-      redis_url: redis://redis:6379/0\n     depends_on:\n-      db:\n-        condition: service_healthy\n-      redis:\n-        condition: service_healthy\n+      - db\n+      - redis\n     ports:\n       - \"8000:8000\"\n-    volumes:\n-      - storage:/data/storage\n-\n   web:\n-    build:\n-      context: ..\n-      dockerfile: theo/services/web/Dockerfile\n+    build: ../services/web\n     env_file: ../.env\n-    environment:\n-      NEXT_PUBLIC_API_BASE_URL: http://api:8000\n-      API_BASE_URL: http://api:8000\n     depends_on:\n-      api:\n-        condition: service_started\n+      - api\n     ports:\n       - \"3000:3000\"\n-\n volumes:\n   db: {}\n-  storage: {}\n\\ No newline at end of file\n",
    "path": "infra/docker-compose.yml",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "Refactor example environment variables for clarity and local development setup\n",
    "commitHash": "93cc7b51d2e05d903f2d4f28544224a25ee7fe79",
    "date": "2025-09-24 13:17:03",
    "diff": "@@ -1,39 +1,3 @@\n-# TheoEngine environment variables (example)\n-# Copy to .env and adjust.\n-# pwsh: Copy-Item .env.example .env -Force\n-\n-# --- API (FastAPI) ---\n-# Local dev: SQLite + local storage directory\n-database_url=sqlite:///./theo.db\n-storage_root=./storage\n-\n-# Broker (only needed if wiring Celery/Redis). For local Redis, use localhost.\n-redis_url=redis://localhost:6379/0\n-\n-# Ingestion/embedding defaults\n-embedding_model=BAAI/bge-m3\n-embedding_dim=1024\n-max_chunk_tokens=900\n-doc_max_pages=5000\n-transcript_max_window=40.0\n-user_agent=TheoEngine/1.0\n-\n-# Optional fixtures path override (auto-detected if ./fixtures exists)\n-# fixtures_root=./fixtures\n-\n-# --- Web (Next.js) ---\n-# Point the UI to the API in local dev\n-NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8000\n-API_BASE_URL=http://127.0.0.1:8000\n-\n-# --- Docker Compose variants (uncomment if using compose) ---\n-# Inside the compose network, reference services by name\n-# NEXT_PUBLIC_API_BASE_URL=http://api:8000\n-# API_BASE_URL=http://api:8000\n-\n-# For API using Postgres and Redis services in compose\n-# database_url=postgresql+psycopg://postgres:postgres@db:5432/theo\n-# redis_url=redis://redis:6379/0\n-\n-# If running API in a container and mounting a data volume\n-# storage_root=/data/storage\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\n+REDIS_URL=redis://redis:6379/0\n+STORAGE_ROOT=/data/storage\n",
    "path": ".env.example",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n",
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "implement ingestion pipeline and hybrid search",
    "commitHash": "8d1ab58da8e55381b94425a7dffc86d6a9c75437",
    "date": "2025-09-23 19:43:10",
    "diff": "@@ -1,35 +1,12 @@\n-\"\"\"Application configuration for the Theo Engine API.\"\"\"\n+\"\"\"Application configuration.\"\"\"\n \n-from functools import lru_cache\n-from pathlib import Path\n-\n-from pydantic import Field\n-from pydantic_settings import BaseSettings, SettingsConfigDict\n+from pydantic import BaseSettings, Field\n \n \n class Settings(BaseSettings):\n-    \"\"\"Runtime configuration loaded from environment variables.\"\"\"\n-\n-    model_config = SettingsConfigDict(env_prefix=\"\", env_file=\".env\", env_file_encoding=\"utf-8\")\n-\n-    database_url: str = Field(\n-        default=\"sqlite:///./theo.db\", description=\"SQLAlchemy database URL\"\n-    )\n-    redis_url: str = Field(default=\"redis://redis:6379/0\", description=\"Celery broker URL\")\n-    storage_root: Path = Field(default=Path(\"./storage\"), description=\"Location for persisted artifacts\")\n-    embedding_model: str = Field(default=\"BAAI/bge-m3\")\n-    embedding_dim: int = Field(default=1024)\n-    max_chunk_tokens: int = Field(default=900)\n-    doc_max_pages: int = Field(default=5000)\n-    user_agent: str = Field(default=\"TheoEngine/1.0\")\n-\n-@lru_cache\n-def get_settings() -> Settings:\n-    \"\"\"Return a cached Settings instance.\"\"\"\n-\n-    settings = Settings()\n-    settings.storage_root.mkdir(parents=True, exist_ok=True)\n-    return settings\n+    database_url: str = Field(env=\"DATABASE_URL\", default=\"postgresql+psycopg://postgres:postgres@db:5432/theo\")\n+    redis_url: str = Field(env=\"REDIS_URL\", default=\"redis://redis:6379/0\")\n+    storage_root: str = Field(env=\"STORAGE_ROOT\", default=\"/data/storage\")\n \n \n-settings = get_settings()\n+settings = Settings()\n",
    "path": "theo/services/api/app/core/settings.py",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\")\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\")\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "scaffold theo engine mvp structure",
    "commitHash": "6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910",
    "date": "2025-09-23 19:17:01",
    "diff": "@@ -1,3 +0,0 @@\n-DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\n-REDIS_URL=redis://redis:6379/0\n-STORAGE_ROOT=/data/storage\n",
    "path": ".env.example",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "scaffold theo engine mvp structure",
    "commitHash": "6ada54db02bd11ac0d5c9ede1f88b8cc20bcf910",
    "date": "2025-09-23 19:17:01",
    "diff": "@@ -1,12 +0,0 @@\n-\"\"\"Application configuration.\"\"\"\n-\n-from pydantic import BaseSettings, Field\n-\n-\n-class Settings(BaseSettings):\n-    database_url: str = Field(env=\"DATABASE_URL\", default=\"postgresql+psycopg://postgres:postgres@db:5432/theo\")\n-    redis_url: str = Field(env=\"REDIS_URL\", default=\"redis://redis:6379/0\")\n-    storage_root: str = Field(env=\"STORAGE_ROOT\", default=\"/data/storage\")\n-\n-\n-settings = Settings()\n",
    "path": "theo/services/api/app/core/settings.py",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\")\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\")\n"
    ]
  },
  {
    "branch": "origin/work",
    "commit": "Add initial blueprint for Theo Engine including mission, architecture, and MVP specifications\n",
    "commitHash": "e78da1f51a29708b06c70056b0485588372e368f",
    "date": "2025-09-23 19:04:10",
    "diff": "@@ -0,0 +1,458 @@\n+# Theo Engine \u2014 Final Build Spec (Standalone)\n+\n+## 0) Mission & MVP\n+\n+Goal: Build a research engine for theology that indexes your library (papers, notes, YouTube transcripts, audio), normalizes Scripture references (OSIS), and gives deterministic, verse-anchored search + a Verse Aggregator across the whole corpus.\n+\n+MVP outcomes (no LLM required):\n+\n+Ingest local files and URLs (including YouTube).\n+\n+Parse to chunked, citation-preserving passages with page/time anchors.\n+\n+Detect and normalize Bible references \u2192 OSIS.\n+\n+Hybrid search (pgvector embeddings + lexical).\n+\n+Verse Aggregator: open any OSIS (e.g., John.1.1) \u2192 see every snippet across the corpus, with jump links (page/time).\n+\n+Minimal web UI: Upload, Search, Verse, Document.\n+\n+Generative answers (RAG) are out of scope for MVP. Keep hooks ready for later.\n+\n+## 1) Repo Layout (monorepo)\n+\n+theo/\n+\u251c\u2500 services/\n+\u2502  \u251c\u2500 api/                 # FastAPI service\n+\u2502  \u2502  \u251c\u2500 app/\n+\u2502  \u2502  \u2502  \u251c\u2500 main.py\n+\u2502  \u2502  \u2502  \u251c\u2500 routes/        # FastAPI routers\n+\u2502  \u2502  \u2502  \u2502  \u251c\u2500 ingest.py\n+\u2502  \u2502  \u2502  \u2502  \u251c\u2500 search.py\n+\u2502  \u2502  \u2502  \u2502  \u251c\u2500 verses.py\n+\u2502  \u2502  \u2502  \u2502  \u2514\u2500 documents.py\n+\u2502  \u2502  \u2502  \u251c\u2500 core/          # db, settings, logging\n+\u2502  \u2502  \u2502  \u251c\u2500 ingest/        # parsers, chunkers, osis detection\n+\u2502  \u2502  \u2502  \u251c\u2500 retriever/     # hybrid search/rerank\n+\u2502  \u2502  \u2502  \u251c\u2500 models/        # pydantic schemas\n+\u2502  \u2502  \u2502  \u2514\u2500 workers/       # Celery tasks\n+\u2502  \u2502  \u2514\u2500 requirements.txt\n+\u2502  \u251c\u2500 web/                 # Next.js 14 (App Router)\n+\u2502  \u2502  \u251c\u2500 app/\n+\u2502  \u2502  \u2502  \u251c\u2500 upload/page.tsx\n+\u2502  \u2502  \u2502  \u251c\u2500 search/page.tsx\n+\u2502  \u2502  \u2502  \u251c\u2500 verse/[osis]/page.tsx\n+\u2502  \u2502  \u2502  \u2514\u2500 doc/[id]/page.tsx\n+\u2502  \u2502  \u2514\u2500 package.json\n+\u2502  \u2514\u2500 cli/                 # optional: bulk ingest CLI\n+\u2502     \u2514\u2500 ingest_folder.py\n+\u251c\u2500 infra/\n+\u2502  \u251c\u2500 docker-compose.yml\n+\u2502  \u251c\u2500 db-init/pgvector.sql\n+\u2502  \u2514\u2500 Makefile\n+\u251c\u2500 docs/\n+\u2502  \u251c\u2500 API.md\n+\u2502  \u251c\u2500 Chunking.md\n+\u2502  \u251c\u2500 OSIS.md\n+\u2502  \u2514\u2500 Frontmatter.md\n+\u2514\u2500 .env.example\n+\n+## 2) Stack\n+\n+Backend: FastAPI (Python 3.11+), Celery workers, Redis broker.\n+\n+DB: Postgres 15 with pgvector + pg_trgm.\n+\n+Parsing: Docling (primary), Unstructured (fallback).\n+\n+Bible refs: pythonbible for OSIS normalization.\n+\n+Embeddings: BAAI/bge-m3 (1024-d) via sentence-transformers/FlagEmbedding.\n+\n+Search: Hybrid = vector ANN (HNSW) + lexical (tsvector) with simple rerank.\n+\n+Frontend: Next.js 14 (App Router), minimal pages.\n+\n+## 3) Ingestion Types (standalone)\n+\n+The engine accepts these source types out of the box. No extension/plugins required.\n+\n+Articles / Papers: .pdf, .docx, .html, .txt\n+\n+Web pages: canonical URL (fetch, sanitize to HTML/text)\n+\n+YouTube: video URL (pull transcript if available; else queue ASR)\n+\n+Audio: .mp3/.wav + optional transcript (.vtt/.srt/.json)\n+\n+Markdown notes: .md with optional YAML frontmatter\n+\n+Bibliography (optional): CSL-JSON for metadata backfill\n+\n+If present, a small frontmatter JSON/YAML block improves provenance/dedupe (see \u00a710).\n+\n+## 4) Infrastructure\n+\n+infra/docker-compose.yml\n+version: \"3.9\"\n+services:\n+  db:\n+    image: postgres:15\n+    environment:\n+      POSTGRES_DB: theo\n+      POSTGRES_PASSWORD: postgres\n+    ports: [\"5432:5432\"]\n+    volumes: [\"db:/var/lib/postgresql/data\"]\n+  redis:\n+    image: redis:7\n+    ports: [\"6379:6379\"]\n+  api:\n+    build: ./services/api\n+    env_file: .env\n+    depends_on: [db, redis]\n+    ports: [\"8000:8000\"]\n+  web:\n+    build: ./services/web\n+    env_file: .env\n+    depends_on: [api]\n+    ports: [\"3000:3000\"]\n+volumes: { db: {} }\n+\n+infra/db-init/pgvector.sql\n+CREATE EXTENSION IF NOT EXISTS vector;\n+CREATE EXTENSION IF NOT EXISTS pg_trgm;\n+\n+.env.example\n+DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/theo\n+REDIS_URL=redis://redis:6379/0\n+STORAGE_ROOT=/data\n+EMBEDDING_MODEL=BAAI/bge-m3\n+EMBEDDING_DIM=1024\n+MAX_CHUNK_TOKENS=900\n+DOC_MAX_PAGES=5000\n+USER_AGENT=\"TheoEngine/1.0\"\n+\n+services/api/requirements.txt\n+fastapi[all]==0.115.*\n+uvicorn[standard]==0.30.*\n+psycopg[binary]==3.*\n+SQLAlchemy==2.*\n+pgvector==0.3.*\n+pydantic==2.*\n+python-multipart==0.0.*\n+celery==5.*\n+redis==5.*\n+docling==2.*            # primary parser\n+unstructured==0.15.*# fallback\n+pythonbible==0.1.*      # OSIS normalization\n+regex==2024.*\n+sentence-transformers==3.*\n+flagembedding==1.*# BGE-M3\n+beautifulsoup4==4.*     # web fetch cleanup\n+yt-dlp==2025.*# fetch YouTube metadata/transcript where allowed\n+youtube-transcript-api==0.6.*  # transcript fetcher\n+webvtt-py==0.5.*# parse VTT\n+pydub==0.25.*           # audio utils (metadata)\n+\n+## 5) Database Schema (DDL)\n+\n+-- documents: one row per source artifact (pdf, url, video, audio, note)\n+CREATE TABLE documents (\n+  id UUID PRIMARY KEY,\n+  title TEXT,\n+  authors TEXT[],\n+  source_url TEXT,\n+  source_type TEXT CHECK (source_type IN\n+    ('pdf','docx','html','txt','url','youtube','audio','markdown','note','ai_summary')),\n+  collection TEXT,\n+  pub_date DATE,\n+  channel TEXT,           -- for YouTube/podcasts\n+  video_id TEXT,          -- platform id\n+  duration_seconds INT,   -- if known\n+  bib_json JSONB,\n+  sha256 TEXT UNIQUE,\n+  storage_path TEXT,      -- path to original or normalized pack\n+  created_at TIMESTAMPTZ DEFAULT now()\n+);\n+\n+-- passages: chunked spans with page or time anchors\n+CREATE TABLE passages (\n+  id UUID PRIMARY KEY,\n+  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n+  page_no INT,            -- for paged docs\n+  t_start REAL,           -- seconds (for A/V)\n+  t_end REAL,             -- seconds (for A/V)\n+  start_char INT,\n+  end_char INT,\n+  osis_ref TEXT,          -- 'John.1.1-5' (nullable)\n+  text TEXT NOT NULL,\n+  tokens INT,\n+  embedding vector(1024),\n+  lexeme tsvector,\n+  meta JSONB              -- e.g., speaker, chapter title\n+);\n+\n+CREATE INDEX ix_passages_embedding_hnsw ON passages USING hnsw (embedding vector_l2_ops);\n+CREATE INDEX ix_passages_lexeme ON passages USING gin (lexeme);\n+CREATE INDEX ix_passages_osis ON passages (osis_ref);\n+CREATE INDEX ix_passages_doc ON passages (document_id);\n+\n+## 6) Chunking & Normalization (algorithms)\n+\n+Document these in docs/Chunking.md & docs/OSIS.md. Implement in services/api/app/ingest/.\n+\n+6.1 Parsing\n+\n+Try Docling first. If it fails or yields empty content, fallback to Unstructured.\n+\n+Preserve page numbers and element coordinates when available.\n+\n+6.2 Chunking rules\n+\n+Target ~900 tokens per chunk; clamp to 500\u20131200.\n+\n+Respect block boundaries (headings, paragraphs, list items).\n+\n+Don\u2019t split a detected OSIS span across chunks if avoidable.\n+\n+For PDFs, keep page_no for each chunk.\n+\n+For transcripts:\n+\n+Segment by sentence timestamps if provided; otherwise by caption blocks, coalesced up to ~40s window.\n+\n+Keep t_start/t_end and a speaker field in meta when recognizable (SPEAKER: prefixes).\n+\n+6.3 OSIS detection\n+\n+Pass 1: regex on Bible book names + chapter:verse patterns (support English book aliases).\n+\n+Pass 2: feed candidates into pythonbible to normalize to OSIS (Book.Chapter.Verse[-Chapter.Verse]).\n+\n+If multiple refs appear, store:\n+\n+osis_ref: minimal covering range,\n+\n+meta.osis_refs_all: full list.\n+\n+For ranges that overlap page/chunk boundaries, duplicate the OSIS on both chunks (it\u2019s okay; Verse Aggregator dedupes later).\n+\n+## 7) Embeddings & Hybrid Retrieval\n+\n+7.1 Embeddings\n+\n+Model: BAAI/bge-m3 (1024-d). Batch 64\u2013128; L2 normalize.\n+\n+Store in passages.embedding. Generate passages.lexeme via to_tsvector('english', text).\n+\n+7.2 Candidate generation\n+\n+If request includes osis: first select all passages whose osis_ref intersects query range (implement osis_intersects(a,b) in Python).\n+\n+Else: gather top-K from vector index (cosine), union with top-K lexical (tsvector ranking/BM25-ish).\n+\n+7.3 Rerank\n+\n+Score = alpha *cosine_sim + (1 - alpha)* lexical_score (default alpha=0.65).\n+\n+Boost passages with matching osis_ref when query includes verses even if text also present.\n+\n+Deduplicate near-duplicates by same document_id and overlapping anchors.\n+\n+## 8) API Contract\n+\n+Document in docs/API.md. Implement in services/api/app/routes/.\n+\n+8.1 Ingest\n+\n+POST /ingest/file \u2014 multipart file (pdf, docx, html, txt, md, vtt, srt, json)\n+Optional frontmatter (JSON). Returns { document_id, status: \"queued\" }.\n+\n+POST /ingest/url \u2014 JSON { url, source_type? }\n+\n+If YouTube: fetch metadata + transcript; create time-anchored passages.\n+\n+If web page: fetch + sanitize \u2192 HTML\u2192text.\n+\n+POST /ingest/transcript \u2014 multipart: transcript (vtt/srt/json), optional audio (mp3/wav), optional frontmatter.\n+\n+POST /jobs/reparse/{document_id} \u2014 enqueue re-ingestion.\n+\n+8.2 Search\n+\n+GET /search\n+Query params: q, osis?, author?, collection?, k?\n+Response:\n+\n+{\n+  \"query\":\"...\",\"results\":[\n+    {\n+      \"document_id\":\"uuid\",\"title\":\"...\",\n+      \"page_no\":12,\"t_start\":123.4,\"t_end\":140.1,\n+      \"osis_ref\":\"John.1.1-5\",\"score\":0.81,\n+      \"snippet\":\"...logos was with God...\"\n+    }\n+  ]\n+}\n+\n+8.3 Verse Aggregator\n+\n+GET /verses/{osis}/mentions\n+Returns all passages whose osis_ref intersects the requested ref/range, with anchors.\n+\n+8.4 Documents\n+\n+GET /documents/{id} \u2014 metadata + list of anchors\n+GET /documents/{id}/passages \u2014 paginated chunks with anchors\n+\n+## 9) Web UI (Next.js 14)\n+\n+/upload \u2014 upload file/URL; show job status.\n+\n+/search \u2014 text box + filters (osis, author, collection). Results grouped by document; anchor links:\n+\n+PDFs \u2192 ?page={page_no}#passage-{id}\n+\n+A/V \u2192 ?t={t_start}s\n+\n+/verse/[osis] \u2014 Verse Aggregator list of mentions; filters by source type/author.\n+\n+/doc/[id] \u2014 simple reader with passages list and anchors.\n+\n+Styling can be minimal (Tailwind optional). SSR fetch from API.\n+\n+## 10) Frontmatter (optional but supported)\n+\n+Accept as JSON (in form field frontmatter) or YAML at top of .md. Example fields:\n+\n+id: \"uuid-v4\"\n+title: \"Did Jesus Claim to Be God?\"\n+source_type: \"youtube\"         # or \"article\" | \"note\" | \"ai_summary\" ...\n+authors: [\"Ehrman, Bart D.\"]\n+channel: \"Bart D. Ehrman\"\n+video_id: \"abc123\"\n+date: \"2021-03-14\"\n+collection: \"Christology/Debates\"\n+tags: [\"Ehrman\",\"Divinity\"]\n+osis_refs: [\"John.1.1-5\",\"Isa.52.13-53.12\"]   # optional hints\n+sha256: \"content hash\"\n+\n+## 11) Workers & Pipeline (outline code)\n+\n+services/api/app/workers/tasks.py\n+\n+from celery import Celery\n+celery = Celery(__name__, broker=\"redis://redis:6379/0\", backend=\"redis://redis:6379/0\")\n+\n+@celery.task(name=\"tasks.process_file\")\n+def process_file(doc_id: str, path: str, frontmatter: dict = None):\n+    # parse -> chunk -> osis -> embed -> upsert\n+\n+@celery.task(name=\"tasks.process_url\")\n+def process_url(doc_id: str, url: str, source_type: str | None = None):\n+    # fetch web/youtube -> normalize -> chunk -> osis -> embed -> upsert\n+\n+services/api/app/ingest/pipeline.py (key steps)\n+\n+def run_pipeline_for_file(doc_id, path, fm):\n+    # 1) detect type by extension; parse (Docling/Unstructured)\n+    # 2) chunk by rules (paged vs transcript)\n+    # 3) detect OSIS -> normalize\n+    # 4) embed (BGE-M3) -> upsert passages + lexeme\n+\n+services/api/app/retriever/hybrid.py\n+\n+def search(q: str, osis: str | None, filters: dict, k: int):\n+    # 1) if osis: pull all OSIS-matching passages (range intersect)\n+    # 2) dense topK + lexical topK\n+    # 3) rerank & dedupe; return merged list\n+\n+## 12) Definition of Done (MVP)\n+\n+Ingest PDF and YouTube URL successfully \u2192 passages created with correct anchors.\n+\n+/search works for:\n+\n+keyword-only,\n+\n+OSIS-only,\n+\n+combined (keyword + OSIS).\n+\n+/verses/{osis}/mentions returns correct list for seeded verse refs.\n+\n+Web UI pages function (Upload/Search/Verse/Doc).\n+\n+Basic persistence of originals + normalized JSON under STORAGE_ROOT/{document_id}/.\n+\n+## 13) Testing & Fixtures\n+\n+Fixtures:\n+\n+fixtures/pdf/sample_article.pdf \u2014 contains a visible verse citation (e.g., \u201cJohn 1:1\u20135\u201d).\n+\n+fixtures/youtube/transcript.vtt \u2014 with speaker tags and a verse mention.\n+\n+fixtures/markdown/notes.md \u2014 with frontmatter + OSIS refs.\n+\n+Tests:\n+\n+Unit: OSIS regex \u2192 pythonbible normalization (edge cases: ranges, multiple refs).\n+\n+Integration: ingest PDF \u2192 search by q=logos osis=John.1.1-5 returns expected doc/page.\n+\n+Verse aggregator: GET /verses/John.1.1/mentions lists \u22651 passage with correct anchors.\n+\n+## 14) Make Targets\n+\n+infra/Makefile\n+\n+up:      ## start all services\n+\\tdocker compose up --build -d\n+down:\n+\\tdocker compose down\n+migrate: ## install extensions\n+\\tdocker compose exec -T db psql -U postgres -d theo < infra/db-init/pgvector.sql\n+logs:\n+\\tdocker compose logs -f api web\n+psql:\n+\\tdocker compose exec db psql -U postgres -d theo\n+\n+## 15) Non-Goals (MVP) & Hooks for Next Sprint\n+\n+Not in MVP: RAG answerer, text-reuse graphs, CollateX alignment, IIIF viewer, OpenAlex/GROBID enrichment, auth/multi-tenant, fine-grained permissions.\n+\n+Keep hooks:\n+\n+passages.meta for future speaker, chapter, osis_refs_all.\n+\n+Worker queue for passim/CollateX tasks.\n+\n+documents.bib_json for later OpenAlex/GROBID enrichment.\n+\n+## 16) Post-MVP Roadmap (toggle-able)\n+\n+Text-reuse: passim over corpus \u2192 \u201cParallels\u201d sidebar.\n+\n+Alignment: CollateX diff view between selected passages.\n+\n+RAG answers: small open-weights model via vLLM (Qwen2.5-32B / Llama-3.1-70B / Mixtral). Strict citation policy: answer only from retrieved passages.\n+\n+Metadata enrich: GROBID + OpenAlex; DOI/venue backfill.\n+\n+IIIF pane: render scanned plates next to normalized text.\n+\n+Auth + collections: user orgs; per-collection indices.\n+\n+## 17) Notes & Guardrails\n+\n+Always store originals and normalized JSON under STORAGE_ROOT/{document_id} for reproducibility.\n+\n+Record parser, parser_version, chunker_version in passages.meta.\n+\n+Keep embeddings L2-normalized; consistent preprocessing (lowercase/strip) before tsvector.\n+\n+Implement a robust range-intersect for OSIS to avoid false misses.\n+\n+Respect robots/ToS for URL/YouTube fetching; cache transcripts; expose a manual \u201cupload transcript\u201d path.\n",
    "path": "docs/BLUEPRINT.md",
    "printDiff": "\u001b[93mpsycopg://postgres:postgres@db:5432/theo\n\u001b[0m",
    "reason": "Password in URL",
    "stringsFound": [
      "psycopg://postgres:postgres@db:5432/theo\n"
    ]
  }
]